---
id: caching-strategies
sidebar_position: 4
title: 缓存策略
---

# 引言:LLM推理中的KV缓存的重要性

大型语言模型(LLM)的推理采用自回归生成方式:模型会根据已有的上下文逐步生成后续token。在这一过程中，每个Transformer解码器层都会针对输入序列的每个token计算"键(Key)"和"值(Value)"向量，并在生成新token时通过注意力机制将新的查询(Query)向量与之前缓存的Key/Value进行计算。为了避免每产生一个新token就重新计算先前所有token的Key/Value，推理框架会将它们存储在**KV缓存**中，以供后续步骤重复使用。这就是KV缓存(键-值缓存)的作用:** 在自回归生成过程中缓存先前步骤的注意力Key和Value张量**，从而加速后续token的推理。

KV缓存对于模型性能至关重要。一方面，缓存的存在使得每生成一个新token只需计算一次前向传播，并利用缓存的KV直接计算注意力，而不必重新处理完整序列，从而**将生成阶段每步的计算复杂度从O(n²)**(重新计算长度为n的序列所有注意力)**降至O(n)**(仅计算与新token相关的注意力)。另一方面，KV缓存本身占用大量显存资源，并随着序列长度增长线性增大。例如，LLaMA-13B模型对单条序列维护的KV缓存可达约1.7GB显存。并且序列长度是动态且不可预测的，这给高效管理KV缓存带来了挑战。许多推理框架都在**缓存管理策略**上下功夫，以在保证模型速度的同时尽量降低KV缓存对显存和计算的消耗。

本文将系统分析主流LLM推理框架(如vLLM、TensorRT-LLM、DeepSpeed、SGLang)中采用的KV缓存管理机制。首先，我们将介绍传统(标准)KV缓存的实现方式及存在的问题；随后对比几种改进的缓存管理策略，包括vLLM的**分页注意力(PagedAttention)**、SGLang的**基数树注意力(RadixAttention)**、以及**多查询注意力(Multi-Query Attention, MQA)**架构等。我们还将通过引用这些项目的源码和文档，剖析核心数据结构和函数实现。最后，我们将比较不同方案在推理吞吐量、显存占用、并发序列数等方面的性能差异，并讨论它们在批量推理、长文本推理、在线服务、多轮对话和逐字流式输出等典型应用场景下的优劣与适配情况。文末我们也会简述这些缓存策略与**FlashAttention**等高效注意力计算内核结合使用的情况。

# 标准KV缓存及其局限

在没有特殊优化的情况下，大部分变换模型推理都会为每个并行推理的序列预先分配一大块连续显存，用于存储该序列在所有Transformer层的Key和Value张量。典型实现如HuggingFace Transformers库，会设定一个最大序列长度上限，并据此分配`[层数 × 批量 × 最大长度 × 隐维度]`大小的张量来容纳KV缓存。随着每个新token生成，其对应的K/V向量会被写入缓存张量的下一个位置。**这种方式简单但存在显著的问题:**

## 主要问题

1. **内存碎片和浪费**:序列长度各不相同且不可事先准确获知，框架不得不按照可能的最大长度分配连续内存。这导致大量未用完的缓存空间被空置保留。例如，为保证不溢出，系统可能为每条请求预留足够容纳最大长度的KV空间，即使实际只用了一部分，剩余空间也不能给其它请求用(过度预留)。又如，当序列结束后，该连续内存直到请求全部完成前都被锁定，无法及时释放给新请求(保留未用)。同时，不同请求预留的固定大块内存由于长度不一，会产生无法利用的间隙(外部碎片)。研究表明，传统KV缓存管理因上述碎片和过度分配问题，**实际有效利用率可能仅20%~40%，高达60%~80%的缓存显存被浪费**。这严重限制了同等硬件上可并发的序列数量和最大可支持的序列长度。

2. **冗余计算:** 在多轮对话或批量推理中，如果多个序列共享相同的前缀(例如共享系统提示语或相同的开头内容)，传统实现通常会各自重复计算并存储各自的KV缓存，无法避免重复开销。这意味着**相同的上下文部分被多次处理**，浪费计算时间。

3. **推理延迟:** 由于缺乏针对并发与流式的优化，传统实现往往需要等待批量中的最长序列处理完毕(或进行填充计算)，或者逐条序列顺序推理，因而**无法充分利用GPU算力**。特别是在在线服务中，不规则的请求长度和并发模式会导致GPU计算资源闲置或低效。

概括而言，标准KV缓存策略在**显存利用和计算重复**方面存在明显不足，成为高效LLM推理的瓶颈。为此，业界提出了多种改进策略来**精细地管理KV缓存内存**和**复用重复计算**，以提升整体性能。下面我们将详细介绍其中几种主要方案。

# 分页缓存(PagedAttention):块级管理与复用

针对传统KV缓存的内存碎片问题，**vLLM提出了分页注意力(PagedAttention)算法**。这一方案借鉴了操作系统虚拟内存分页的思想，将每条序列的KV缓存切分为若干**小的固定大小块(block)**。每个块可以看作能够存储预定数量(如128个)token的K/V向量的"页框"。这些块在显存中不需要彼此连续，可以按需动态分配和释放。vLLM为此维护了一个**块表(Block Table)**数据结构来映射序列的逻辑位置到物理内存块。可以类比:** 序列的连续token序列相当于进程的虚拟地址空间，块相当于内存页，块表则类似页表**。通过块表，序列的逻辑KV缓存块可以灵活地映射到任意物理显存地址的块上，并支持按需增长。

## 实现细节

在vLLM中，每条序列都有一个`BlockTable`对象，内部保存一个块列表(每个块对应一段物理显存)和块大小等信息。当一个新token产生时，若当前块未填满则直接写入；若当前块已满，则向块表申请下一个空闲块用于存储后续token。vLLM通过`DeviceAwareBlockAllocator`等组件负责在GPU显存中分配/释放这些块，并跟踪哪些块空闲或已占用。块大小(例如128或256个token)是分页的粒度，可在一定范围内调整:** 较小的块降低内存浪费但增加管理开销，较大的块减少管理开销但可能增加未用尾部空间**。vLLM团队选择了128作为经验折中值，并指出分页导致的内存浪费仅发生在每个序列的最后一个未填满块上。实际测量表明，PagedAttention使得KV缓存的内存利用率高达96%以上，浪费不到4%。相比传统实现高达60-80%的浪费，**内存效率大幅提升**。

更灵活的内存管理带来了**显著收益**:几乎所有预留的KV缓存显存都被有效利用，可以在同样GPU上并发处理更多序列或更长的序列，从而**提高吞吐量**。vLLM的实验显示，使用PagedAttention后在NVIDIA A100等GPU上对LLaMA模型的吞吐量比未优化的HuggingFace实现提升了14～24倍；相较之前业界最快的TGI服务器也有约2.5～3.5倍提升。这些提升主要归功于**批处理调度与KV内存高效利用**带来的GPU利用率改进。

## 缓存复用机制

除了内存碎片优化，PagedAttention还自然地支持**跨请求共享KV缓存块**。如果多个序列在前缀部分有完全相同的内容，vLLM不会重复保存多份KV数据，而是让它们的块表条目指向同一物理块，实现前缀的"一份计算，多处复用"。例如在**并行采样**场景下，从同一个prompt生成多个不同的输出时，prompt部分的KV计算可共享；又如**Beam Search**中多个beam序列在未分叉前共享相同前缀，其KV亦只保留一份。为了保证共享的安全性，PagedAttention采用了与操作系统相似的**引用计数+写时复制(Copy-on-Write)**机制:块表维护每个物理块的引用计数，表示有多少序列在使用该块；当某个序列需要在共享前缀基础上继续追加新token且可能修改该块内容时，系统会检测到该块已被其他序列引用，从而为该序列分配一个新块(拷贝原内容)来写入，以免影响其他序列已生成的内容。通过这种方式，vLLM成功实现了**前缀KV缓存的自动共享与复用**(Automatic Prefix Caching)，避免了重复计算，大幅加速了多轮对话、并行生成等场景下的**首token延迟**和总体吞吐。

## 广泛影响

PagedAttention作为一项开创性的缓存管理技术，已被其它推理框架广泛借鉴和支持。例如，HuggingFace的TGI后续引入了类似分页机制减少KV浪费，NVIDIA的TensorRT-LLM也在其架构中实现了paged KV管理。下文将介绍TensorRT-LLM的做法。总的来说，PagedAttention通过**块级管理与按需分配**解决了KV缓存内存利用率低的问题，并提供了机制高效地复用缓存内容，对于高吞吐LLM服务奠定了基础。

# 基数树缓存(RadixAttention):基于前缀树的自动缓存复用

尽管PagedAttention极大提高了KV缓存的内存利用率并支持共享**完全相同**的前缀块，但它对前缀复用的匹配粒度仍然是**块级且要求精确匹配**。这意味着只有当请求的前缀在块划分边界上完全一致时，才能直接共享缓存；对于部分重叠但不完全相同的前缀或**非块对齐**的重叠，复用仍不够灵活。另外，PagedAttention的前缀共享在实践中往往需要开发者**提前规划批次**(例如把前缀相同的请求集中处理)才能充分发挥作用，否则可能出现因为并行调度不同步而无法及时复用的情况。

为了解决上述问题，SGLang框架提出了**RadixAttention(基数树注意力)**。RadixAttention通过在后端维护一个**前缀基数树(Radix Tree)**来自动管理和复用KV缓存，从**token级粒度**实现灵活的前缀匹配。具体来说，RadixAttention会将所有经过的prompt(包括每轮对话的历史)映射到一棵前缀树结构中，其中每条边代表一段token序列，节点对应某个前缀并存储该前缀对应的完整KV缓存张量引用。当一个新请求到来时，SGLang运行时首先在这棵树上执行前缀匹配，找到与该请求开头最长的公共前缀节点，并**重用**其已缓存的KV张量。余下的新token部分则正常通过模型计算，其结果KV追加到树中形成新的分支节点。这样，即使两个请求只有部分重叠的前缀(例如系统提示相同但用户提问不同)，RadixAttention也能复用公共前缀段的缓存，避免重复计算**这段公共上下文**。相比之下，vLLM的块级缓存复用要求整个前缀序列完全相同才能共享缓存，而无法处理"部分匹配"的情形。

## 实现特点

RadixAttention的数据结构采用了压缩前缀树(Radix Tree)而非普通Trie，可以在单条边上标记一串token子序列，从而更高效地表示和查找前缀。每个树节点关联对应前缀的KV缓存张量(这些张量仍存放在GPU显存中，且**底层采用分页布局**，每个token一个"页"类似vLLM的块)。整棵前缀树则保存在CPU内存中进行管理，插入和查找操作的开销很小。当缓存占用接近显存上限时，SGLang会根据**最近最少使用(LRU)**策略从树的叶子开始逐步淘汰不常用的前缀节点及其KV缓存(释放显存)。为了最大化缓存命中率，SGLang还实现了"缓存感知"的调度策略，尽可能让共享前缀的请求临近执行，从而在缓存被淘汰前得到复用机会。这一系列机制都是全自动的，无需用户干预配置。正因为RadixAttention能**自动识别和缓存部分重叠的上下文**，非常适合多轮对话这种上下文不断增长、部分重复的场景。在这些动态交互场景下，vLLM尽管具备前缀缓存能力，但往往需要人工将prompt模板化、批量整理等"静态优化"才能充分发挥作用；而RadixAttention则体现出"动态优化"的优势——无需预先假定请求模式，能够自适应地发现缓存复用机会。

## 性能优势

RadixAttention对多轮对话等场景的提速效果在实测中相当显著。例如，有报告对一个包含7k上下文token的多轮会话进行了对比测试:使用RadixAttention缓存命中后，生成首个新token的延迟比首次无缓存时降低了约20%；整体生成速率约提升到每秒35个token，比vLLM的块级前缀缓存在相同场景下(约33个token/s)**高出约10%**。当上下文重复度越高、对话轮数越多，这种优势越明显。在复杂对话、Agent、多段推理程序等需要频繁调用LLM的任务中，SGLang据称相较其它系统可实现最高5倍的端到端吞吐提升(这不仅归功于RadixAttention，也包括其调度器、前端DSL等综合优化)。因此，对于**多轮对话共享上下文**这一愈发常见的用例，RadixAttention提供了极具吸引力的缓存管理方案，被认为是选择SGLang作为推理引擎的一大理由。

## 源码解析

SGLang已经开源，其后端实现中RadixAttention相关模块维护了上述前缀树和缓存映射。它充分借鉴了vLLM的分页缓存思想(SGLang同样支持分页KV缓存和连续批处理等功能)，在此基础上增加了前缀树索引机制。SGLang的开发者在项目中引用并复用了vLLM等项目的部分代码来实现这些功能。例如，在前缀匹配的算法上，通过树结构的`insert`和`search`函数快速完成KV缓存的复用查找；在缓存替换上，通过维护每个节点的访问时间戳实现LRU淘汰(递归移除叶子节点)。得益于在CPU上的轻量树操作和GPU上的高效分页缓存，RadixAttention运行开销很低，相比计算LLM本身可忽略不计，但带来的加速却很明显。RadixAttention也与其它优化技术兼容:例如SGLang支持与PagedAttention(分页内存管理)配合使用，以及和连续批调度、张量并行等并行手段共同发挥作用。

## 应用场景

综合来看，**vLLM的块级前缀缓存**(APC)适合前缀模式固定、可以批量优化的场景，例如**批量推理**中很多请求共享相同提示模板，此时vLLM的精细控制和预分组策略能取得非常好的缓存命中率；而**SGLang的RadixAttention**擅长处理动态、复杂的多轮交互场景，在无法预知请求模式或前缀经常部分重复的情况下，Radix机制可以自动地挖掘缓存复用机会，无需人工干预。因此，如果业务以**长对话、多轮问答**为主，SGLang往往能以更低的延迟提供响应；如果是**大批量的独立请求**且可以人为设计统一的prompt模板，vLLM在极致并发吞吐上也有优势。两者各有适用之处，开发者可根据工作负载特征选择合适的框架或结合二者优点。

# 多查询注意力(MQA)与其它缓存优化

上述PagedAttention和RadixAttention属于**系统层面的缓存管理策略**，通过改变内存布局或增加数据结构来提升KV缓存利用率和复用率。除此之外，还有一些从**模型结构或算子层面**改进KV缓存效率的方法。其中最重要的就是**多查询注意力(Multi-Query Attention, MQA)**和**组查询注意力(Grouped-Query Attention, GQA)**。这是Transformer架构的一种变体，**直接减少了每层需要存储的K/V向量数量**，从源头上缓解KV缓存的内存压力。

在标准**多头注意力(MHA)**中，每个注意力头都有自己独立的一组Key和Value投影参数，因此每个token会产生`num_heads`份K和V向量，KV缓存大小随注意力头数线性增长。而**多查询注意力(MQA)**的核心思想是:*让所有注意力头共享同一组Key/Value*。换言之，尽管仍有多个查询头(每个头各自计算Query向量并产生注意力权重)，但这些查询头在计算注意力时**面对的是相同的一套Key向量**，并且在根据注意力权重提取Value时**使用的也是同一套Value向量**(每个头由于注意力分布不同，最终提取出的加权值可能不同，但底层Value是共享的)。通过这种结构，每个token只需在KV缓存中存储**单份K和V**，而不是多份，从而将KV缓存占用降低了约`1/num_heads`(例如有16个头则减少到原来的1/16)。**组查询注意力(GQA)**是对MQA的进一步泛化:不是让所有头都共享一个KV，而是把注意力头分成若干组，每组共享一套Key/Value。比如将32个头分成4组，则每组8个头共享一套KV，则每token缓存KV数量为4份(比原32份减少了87.5%)。通过调整组的大小g，可以在**传统多头(g = num_heads)**和**极端MQA(g = 1)**之间权衡模型表示能力与缓存开销。研究指出，直接用MQA(相当于g=1)对小模型影响尚可，但对大模型可能过于激进，GQA提供了中间地带来平滑地削减KV开销。

## 模型与支持

Multi-Query Attention最早由Shazeer等人在2019年的研究提出，并被一些大型模型所采用。例如，Google的PaLM系列、TII的Falcon模型、Meta的Llama-2 70B，以及近期的Mistral-7B模型等都使用了MQA或GQA结构。这些模型在不严重牺牲精度的前提下，大幅减少了推理时KV缓存的内存占用，因此更易于部署长上下文或多并发的服务。对于推理框架来说，MQA/GQA主要体现为注意力计算的实现区别:需要支持一个查询对应一组(或单个)K/V的算子实现。NVIDIA的TensorRT-LLM等已经原生支持了MHA/MQA/GQA三种注意力模式，在其GPU内核中通过参数调整即可处理不同头数配置。具体而言，在TensorRT-LLM的`gpt_attention`算子中，设定`num_kv_heads`小于`num_query_heads`即可指定使用MQA或GQA计算路径；它针对短序列和长序列分别选择普通算法或FlashAttention算法，以确保高效。因此，对于具有MQA结构的模型，主流框架通常都能够高效运行并充分利用其KV缓存减负优势。需要注意的是，MQA/GQA**本质是模型结构改动**，需要在模型训练时就定型，并非推理时可以自由切换。然而，它为缓存管理开辟了新的路径:** 从源头减少缓存总量**。在上文提到的各种系统优化之外，MQA是一种Orthogonal(正交)的手段，可与分页缓存或Radix前缀缓存**叠加**使用。举例来说:一个采用MQA的Llama-2 70B模型，其KV缓存相当于原始Llama-2 70B的八分之一左右；结合PagedAttention后，这有限的缓存又能以96%的效率利用；再配合RadixAttention，则重复上下文几乎不增加额外计算。可见，多层次的优化能够**协同**提升推理效率。

## KV缓存其它优化

除了以上策略，值得一提的还有**KV缓存的量化和异构内存利用**。由于KV缓存属于中间激活而非模型权重，传统的模型量化(如GPTQ、AWQ)不直接作用于它。但一些推理系统开始支持对KV缓存动态执行低精度存储:例如，FlexGen将KV缓存和模型权重都以4-bit存储在CPU/磁盘以节省GPU显存；NVIDIA TensorRT-LLM提供了配置选项，将KV缓存量化为INT8或FP8格式在GPU上计算和存储；vLLM自0.3.0版本也支持将KV缓存压缩为FP8精度。由于LLM推理往往受内存带宽限制(而非计算吞吐限制)，降低KV缓存精度带来的主要好处是减少显存占用和内存传输量，从而**间接提高速度**。实践中8-bit量化KV通常对生成质量几乎没有影响，因此是"低成本"的优化。再者，在超长上下文(如上万token)推理时，即使有分页管理，单卡GPU的显存可能仍然不够。为此，DeepSpeed等系统引入了**KV缓存异地存储(Offloading)**技术:即将不常用的KV片段临时转移到CPU内存甚至NVMe SSD上，以腾出GPU空间处理当前计算。微软的ZeRO-Inference便利用权重量化+KV缓存CPU Offload，实现了在同样GPU上**4倍批量**处理，从而推理吞吐提升达20倍之多(针对大批并发、小生成长度的场景)。当然，Offload会带来额外的PCIe传输开销，但在某些架构下这很小。例如NVIDIA Hopper架构(H100、GH200等)GPU与主机内存带宽大增，官方报告显示将未命中(暂不需要)的KV块优先移至CPU**Pinned内存**存放，可极大提高缓存复用概率而几乎不增加延迟。TensorRT-LLM允许用户配置一个CPU缓存区大小，如分配45GB主机内存用于存放溢出的KV块。当GPU内存紧张时，低优先级的KV块就移至该缓冲区；若后续又需要用到，再从CPU拷回GPU。有实验表明，在H100等新GPU上启用KV offload可以**最多将首token延迟降低14倍**(主要针对长序列初始处理阶段)。不过在较老的PCIe GPU上此效果有限甚至适得其反。总的来说，**KV缓存量化**和**异构内存管理**为超长文本支持和大批量并发提供了实用手段，目前已在DeepSpeed、TensorRT-LLM等框架中作为可选优化。

# 性能对比与场景分析

不同KV缓存策略对推理性能的影响可以从**吞吐量、显存占用和延迟**等方面考察。下面通过一个表格总结主要机制的特点和性能表现，并结合典型场景进行分析:

| 策略/机制 **(缓存管理)**                   | **主要思想**                                         | **优缺点与适用场景**                                                                | **性能影响**                                                                                          |
| ---------------------------------- | ------------------------------------------------ | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| **标准KV缓存** (常规实现)                  | 每序列预留连续内存存KV，长度按最大值填充，逐token写入                   | 实现简单，但内存碎片和浪费严重；无法自动复用相同前缀；批处理需填充对齐。适用于小模型或低并发等对性能要求不高情况。                   | 有效显存利用率仅20%\~40%；重复前缀需重复计算，吞吐低下；批中短序列因填充导致额外计算。                                                   |
| **分页注意力** (PagedAttention, vLLM等)  | 将每序列KV拆分为固定大小**块**，用块表映射逻辑序列到物理内存；按需分配回收块；可共享相同块 | 极大减少内存碎片，近乎最优利用显存；支持完全相同前缀的缓存共享；实现较复杂，需专用内核支持。适合高并发批量请求、模板化prompt等场景。       | 显存浪费&lt;4%；同等硬件下并发吞吐提升数倍(vLLM比HF提升24x)；prefix命中可零开销复用(需精确匹配前缀)。首token延迟略有增加(需维护块表)，但可接受。             |
| **基数树缓存** (RadixAttention, SGLang) | 维护前缀树索引，实现**任意公共前缀**缓存复用；自动插入/查询，LRU淘汰           | 能自动缓存部分重复上下文，无需人工批处理优化；对多轮对话、复杂链式调用效率高；占用部分CPU内存存树结构。适合聊天机器人、Agent等上下文演化场景。 | 多轮对话首token延迟降低\~20%；缓存命中时比块级策略快约10%；极端情况下可达数倍吞吐提升。占用少量CPU做前缀匹配(开销小于模型计算)。                         |
| **多查询注意力** (MQA/GQA, 模型结构)         | **减少KV头数**:多个注意力头共享一套Key/Value，使每token仅存1份KV     | 从源头减少KV大小，显存占用随头数减少；需模型训练支持，可能略降精度(可用GQA平衡)。适用于大模型部署、长上下文存储受限场景(不少前沿模型已采用)。 | KV缓存内存≈原来的1/num\_heads；注意力计算略减慢(需处理组共享)，但总体推理速度常受内存带宽制约，故影响不大。已在PaLM等验证几乎无性能损失换取巨量内存节省。           |
| **KV缓存量化** (8-bit/4-bit)           | 将KV张量用低精度表示(如INT8、FP8或更低)                        | 大幅压缩缓存大小，降低显存和带宽占用；代价是增加少量量化/反量化算子开销。适用于**内存受限**且**对极致性能要求不高**的部署。          | FP16→INT8将KV容量减半，带来近乎对应的序列并发提升；实际测得对吞吐有显著提升。精度损失可忽略(因KV只是中间值)。例如vLLM 0.3.0引入FP8缓存支持，满足高效同时保持模型质量。 |
| **缓存Offloading** (异构内存)            | 将不活跃的KV缓存**暂存CPU内存/磁盘**，腾出GPU空间                  | 可突破GPU显存限制支持超长上下文或超大并发；需要高速IO支撑，否则可能拖慢速度。适用于离线批处理或对**最大上下文长度**要求极高的场景。      | 在H100等平台，offload未损失明显速度可扩展4×批量；TTFT在特定情况下提升5\~14倍。老旧PCIe系统收益有限。需要预分配固定大内存缓冲。                      |

从上表可以看出，不同机制各有侧重，往往可以组合使用以取得更佳效果。下面结合**典型推理场景**，讨论这些策略的取舍:

* **批量推理(Batch Inference):** 当同时请求数量很多且**共享相同的prompt模版**时，vLLM的PagedAttention结合其**连续批调度**最为有效。它能高效打包并行计算，避免填充浪费，并利用块共享让所有请求共用那段模版前缀的KV计算。这正是vLLM相较传统实现吞吐暴增的重要原因。若批内请求前缀各异，RadixAttention也能发挥作用，但在高度结构化/相同的批场景，块级缓存已足够发挥效果而开销更低。DeepSpeed的ZeRO-Inference则通过8-bit权重+KV下放CPU，实现单机更大批量的推理并行，但其主要收益在显存节省而非减少计算。

* **长文本/长上下文推理:** 长上下文下，首先模型计算瓶颈会转向注意力Softmax开销，许多框架都会启用**FlashAttention**等优化内核(详见下节)。在缓存方面，长上下文意味着KV缓存占用剧增，分页管理的优势在此充分体现:它避免了为最大长度预留整块内存导致的大量浪费。同时，如果上下文超过GPU容量，offload技术可以保持推理能够进行(代价是略增延迟)。一些特殊模型采用**滑动窗口注意力**等机制将关注范围限制在最近的N个token，从而天然控制了KV缓存长度(比如Mistral-7B将注意力窗口设为4096)；这种情况下实际缓存不会无限增长，PagedAttention的优势相对削弱，但RadixAttention依然有意义——例如多段长文拼接的场景，共享的起始段可以缓存一次多用。

* **在线服务(多用户交互):** 在线服务通常面对**不可预测的请求模式**:既可能有大量独立请求并发，也可能有用户与机器人长时间多轮对话。针对前一种情况，vLLM的高吞吐特性保证了GPU资源充分利用，即使请求长度分布不均也能通过连续批处理最大化吞吐。针对后一种情况(多轮对话为主)，SGLang的RadixAttention能显著降低每轮对话的边际延迟，因为系统提示和历史聊天内容大部分在首轮后即缓存，可供后续轮次直接使用。因此如果服务场景偏**聊天助理**类，SGLang往往能以较低延迟支撑更多并发对话；如果是偏**独立问答**类的高并发短问答，vLLM等框架可能以更高总吞吐取胜。TensorRT-LLM则提供了多种模式(如其KV缓存复用开关和并行选项)供用户在服务部署时调优，以匹配不同的工作负载需求。

* **逐字流式输出(Token Streaming):** 流式输出要求模型尽快产生第一个token并持续以稳定速度吐出结果token。KV缓存对于实现流式尤为关键——**没有缓存，共振每生成一个token就要重复处理已有上下文，延迟将随序列增长线性增加**，流式将不可用。得益于KV缓存，生成阶段每个token的计算量保持恒定，使得持续生成得以流畅进行。各框架在这方面的区别主要体现在**首token延迟(TTFT)**和**后续稳定速率**上。通常，vLLM和SGLang由于需要维护更复杂的缓存结构(块表或前缀树)，首token的准备稍有开销，但一般仍在毫秒级别，可通过高并发摊薄。NVIDIA的TensorRT-LLM通过高度优化的kernel减少了预处理开销，并支持在context阶段就使用FlashAttention加速长prompt的处理。有报告称，通过KV缓存offload技巧，TensorRT-LLM在x86服务器上TTFT可降低数倍。一旦进入流式生成阶段，各框架的单步延迟差别不大，因为都做到了用缓存高效查表计算注意力。值得一提的是，vLLM和SGLang都支持**一边生成一边输出**，并在后台持续将新token的KV追加到缓存(这正是它们设计的初衷)。因此在流式场景下，**缓存机制是保障流式性能的基石**，不同框架更多是调度策略和实现优化的不同，缓存本身的作用是一致的。

# 与FlashAttention等优化的结合

无论采用哪种KV缓存策略，**注意力计算本身的效率**同样直接影响推理性能。FlashAttention是一种高效的注意力实现方法，利用显存读写算子融合、块稀疏等技巧，在**减少显存占用**的同时**加速注意力Softmax计算**。主流推理框架普遍将FlashAttention整合为注意力计算的后台实现。例如，TensorRT-LLM在序列较长时自动使用FlashAttention算法处理Q\*K^T和Softmax，从而优化上下文阶段的性能。SGLang则与自研的**FlashInfer**内核深度集成——FlashInfer可以视作FlashAttention的定制版，针对特定硬件和批处理模式做了优化。据文献报道，SGLang通过结合RadixAttention与FlashInfer，在它主要的推荐场景下加速了长序列和大批量的推理。同样地，vLLM的注意力kernel也是专门设计的(在其源码`attention_kernels.cu`中)，以兼容分页缓存布局并高效地将全局内存的数据读入共享内存进行计算。这表明vLLM实际上实现了一套等价于FlashAttention的内核用于单步解码。总结而言，**缓存策略和高效注意力内核是相辅相成的**:前者解决"存什么、如何存"的问题，后者解决"算得快、算得省内存"的问题。两者结合才能充分发挥硬件潜能。例如，TensorRT-LLM报告其在H100 GPU上利用FP8精度和Flash注意力，可达到单卡每秒生成一万余token的速度，同时首字输出延迟仅百毫秒级。这背后正是多种优化的集成。

最后值得指出，缓存管理也影响着一些新兴的推理技术，如**Speculative Decoding(猜测解码)**。后者让一个小模型先行生成若干token并尝试被大模型验证接受，从而跳跃性加速生成。为了回退不接受的token，小模型产生的那部分KV需要暂存和管理。像vLLM和SGLang都已经在其框架中考虑了这种机制(vLLM插件支持猜测解码，SGLang支持多种解码策略混合调度)，这也离不开高效的KV缓存管理配合。可以预见，随着LLM推理场景愈发多样和复杂，缓存策略也将持续演进，包括层级缓存、分布式缓存等都会成为研究热点。但无论如何，其核心目标始终如一:** 充分挖掘历史计算的重用价值，降低内存和计算冗余**，从而以更低的代价、更高的速度去服务日益增长的模型推理需求。各大框架围绕KV缓存展开的系列创新，为我们朝这个目标迈出了扎实的一步。