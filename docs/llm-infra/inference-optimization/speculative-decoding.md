---
id: speculative-decoding
sidebar_position: 5
title: 推测解码
---

## **执行摘要**

推测解码(Speculative Decoding, SD)已成为加速大型语言模型(LLM)推理的关键技术，同时不牺牲其输出质量。其核心机制涉及一个较小的“草稿模型”和一个较大的“目标模型”的协同工作，通过并行验证草稿模型生成的多个令牌，显著提升了推理效率。

这项技术带来了显著的推理延迟改进，通常可实现2到3倍的加速，在某些高级变体中甚至可达7.2倍 1。这种加速直接转化为硬件需求减少，从而降低了运营成本和能耗 1。推测解码的演进历程从其开创性概念发展到多样化的现代变体，涵盖了不同的草稿生成和验证策略 4。

尽管推测解码优势显著，但它也面临一些挑战，尤其是在处理大批量推理时的吞吐量限制 5。当前的研究正致力于克服这些限制，并探索其在多模态应用等新兴领域的潜力 4。

## **1\. 大语言模型推测解码引言**

### **1.1 大语言模型推理延迟的挑战**

大型语言模型(LLM)是现代人工智能领域的核心驱动力，但其庞大的规模导致了高昂的推理延迟，这严重阻碍了实时应用的用户体验 1。这种延迟的根本原因在于LLM的自回归特性。LLM逐个令牌生成输出，每个新令牌的生成都依赖于之前已生成的所有令牌。这种固有的“自回归瓶颈”意味着模型必须为生成一个完整的句子而多次运行，从而导致串行计算的效率低下 1。

此外，推理过程还面临显著的内存瓶颈。每个解码步骤都需要读取模型的全部权重，这对于大型LLM而言可能达到数TB的数据量。这种重复的数据读取使得操作成为内存密集型，而非计算密集型，从而导致并行硬件的计算资源未被充分利用 1。现代硬件，如TPU和GPU，拥有强大的并行计算能力，但在LLM推理中，这些计算单元往往处于空闲状态，等待数据加载。推测解码通过允许并行验证多个令牌来解决这一问题，它旨在在每次内存访问中为昂贵的目标模型提供更多工作，从而最大限度地提高硬件利用率和效率。因此，推测解码所解决的问题不仅仅是“慢”，更是LLM与现代硬件交互时的“低效”，这直接转化为成本节约和环境效益 1。

### **1.2 推测解码的核心概念与动机**

推测解码(SD)是一种创新技术，旨在通过并行生成多个令牌来加速LLM推理，同时不损害输出的质量或统计分布 1。其概念灵感来源于计算机体系结构中的“推测执行”，即预先执行操作并在之后进行验证 1。

推测解码的主要目标是减少“每输出令牌时间”(Time Per Output Token, TPOT)和整体推理延迟，从而使LLM更适用于实时、面向用户的应用 1。虽然直接效益是更快的推理速度 1，但研究材料反复提及“更便宜的推理”和“能源成本降低” 1。这不仅仅是一个附带效应，而是硬件利用率提高的直接结果。如果模型能够在相同硬件上每秒处理更多令牌，那么服务相同流量所需的机器数量就会减少。因此，推测解码从单纯的性能技巧转变为大规模LLM部署的关键经济和环境优化方案，使其在行业中广受欢迎 1。

### **1.3 草稿模型与目标模型的基本工作原理**

推测解码的核心架构涉及两个不同的LLM: 一个较小、速度更快的“草稿模型”和一个较大、更准确的“目标模型” 2。

这两个模型的角色分工明确: 

* **草稿模型**: 其主要职责是快速生成一系列“推测性”或“候选”令牌 5。草稿模型的速度至关重要，而其语言建模能力与推测解码的性能增益并不强相关 7。  
* **目标(验证器)模型**: 负责并行验证这些草稿令牌 4。它确保输出的统计一致性和质量，充当最终的仲裁者。

推测解码的关键加速来源于目标模型在单次前向传播中并行验证多个令牌，而非逐个生成令牌 1。

## **2\. 算法机制与保证**

### **2.1 令牌生成与验证的详细步骤**

推测解码的流程通常始于目标模型自回归地生成第一个令牌 5。随后，当前序列(包括输入和已生成的令牌)被传递给较小的草稿模型，该模型迅速生成

*n* 个候选令牌 5。

这些 *n* 个草稿令牌接着被送入较大的目标模型。目标模型对结合了原始上下文和 *n* 个草稿令牌的序列执行单次前向传播，以并行验证它们 2。目标模型评估每个草稿令牌，并接受与其自身分布一致的令牌前缀。如果某个草稿令牌不一致，则该令牌及其后续所有草稿令牌都将被拒绝 5。

如果令牌被拒绝，目标模型将从最后一个被接受的令牌开始，自回归地生成一个新的令牌。此过程随后循环，草稿模型根据更新后的序列生成新的候选令牌 5。这个过程并非简单的并行生成，而是一种复杂的“猜测与检查”机制 5。草稿模型进行廉价的猜测，而昂贵的目标模型则执行单次并行验证。效率的提升来源于接受令牌数量与目标模型昂贵的前向传播次数之间的比率。如果每次前向传播都能接受大量令牌，则每个令牌的有效成本将显著下降。这突出了草稿模型在预测目标模型输出方面的准确性至关重要，而不仅仅是其速度。一个表现不佳的草稿模型会导致频繁的拒绝，从而抵消推测解码带来的效益 11。

### **2.2 草稿模型与目标(验证器)模型的角色**

* **草稿模型(提议者)**:   
  * **速度优先于质量**: 优先考虑快速生成。其延迟是整体性能的关键因素 5。  
  * **候选生成**: 提出一系列可能由目标模型生成的令牌序列 5。  
  * **轻量级**: 通常是小得多的LLM(例如，用于Llama 3.1-70B的Llama 3.1-8B)或专门的低复杂度网络 5。  
* **目标模型(评分器/验证器)**:   
  * **质量保证**: 确保最终输出与目标模型自回归生成的结果完全一致 1。  
  * **并行验证**: 其独特作用是在单次前向传播中同时评估多个草稿令牌 2。  
  * **权威来源**: 它是令牌分布的“真实来源” 8。

### **2.3 无损保证与输出质量保持**

推测解码的一个关键优势在于它保证输出令牌与未经修改的目标模型自回归生成的结果“完全相同” 1。这通过一种称为拒绝采样(rejection sampling)的机制实现 5。例如，vLLM明确声明了理论上的无损性(受限于硬件数值精度)以及通过拒绝采样收敛性测试和贪婪采样等效性测试进行算法验证 13。

这种无损特性对于生产环境至关重要，因为它意味着性能提升不会以模型保真度为代价，从而维护了用户信任和应用程序的可靠性。研究材料中反复强调“不牺牲质量”和“保证输出分布一致” 1，这不仅仅是一个技术细节，更是其被行业广泛采用的关键因素。在许多应用中(例如法律、医疗、金融)，即使与原始模型输出的微小偏差也可能导致严重后果。这种无损保证使得推测解码成为一种“安全”的优化方法，允许公司将其部署到对模型输出完整性要求极高的敏感应用中。这有助于建立对该技术本身的信任，从而加速其在关键系统中的集成，与量化或剪枝等通常涉及质量权衡的优化技术形成对比 6。

## **3\. 推测解码技术的演进**

### **3.1 开创性概念与基础论文**

推测解码的演进可以追溯到早期的开创性工作: 

* **起源(2018): 块级解码(Blockwise Decoding)**: 由Stern等人于2018年提出，是推测解码的早期前身。该方法在Transformer解码器之上引入了额外的全连接前馈网络(FFN)头部，以同时生成多个令牌，然后由原始LLM并行验证 4。  
* **基础论文(2022): 《通过推测解码实现Transformer的快速推理》**: 由Yaniv Leviathan、Matan Kalman和Yossi Matias(Google Research)撰写，这篇论文正式引入了推测解码技术。它展示了在翻译和摘要等任务中显著的推理速度提升(约2-3倍)，同时保证了输出分布的一致性 1。  
* **同期贡献(2023): 推测采样(Speculative Sampling)**: Leviathan等人(2023年)和Chen等人(2023a年)将该范式扩展到包括各种采样方法的无损加速，通常使用较小的现成语言模型作为草稿模型，从而无需额外训练 4。

从“块级解码”(一种特定的架构修改)到“推测解码”和“推测采样”(研究材料中描述的更通用的“范式”或“技术” 4)的演进，标志着该领域的成熟。最初的解决方案与特定的模型架构紧密耦合(例如，Transformer解码器上的FFN头部)。后来的工作将其泛化，表明“草稿-验证”的核心思想可以广泛应用，甚至适用于现成的模型。这种泛化显著降低了采用门槛，使得更广泛的研究人员和实践者无需进行深层次的架构修改或大量再训练即可实现和实验推测解码。它将推测解码从一个利基优化转变为一个广泛适用的框架。

### **3.2 关键进展与范式转变**

* **SpecDec(2023)**: Xia等人(2023年)引入了SpecDec，它使用一个独立的、专门的非自回归Transformer作为草稿模型。该研究还探索了放宽严格的验证标准以提高接受率，实现了约5倍的加速 4。  
* **2022年后的普及**: 推测解码正式引入后，获得了广泛关注，导致了各种变体和进展的迅速涌现，包括SpecInfer、Assistant Generation、PPD、SpecTr、StagedSpec、Medusa、Draft & Verify、BiLD、LLMCad和EAGLE 4。  
* **关注硬件效率**: 最近的工作强调设计“硬件高效的草稿模型” 7和优化大批量推理时的吞吐量，这表明研究重点已从单纯的延迟降低转向更广泛的成本和可扩展性问题 14。

研究材料显示，该领域呈现出清晰的趋势: 最初关注核心算法思想，随后迅速扩展到多样化的算法变体 4。与此同时，对“硬件高效的草稿模型” 7、“编译器和硬件优化” 6以及与vLLM等服务框架集成 10的关注日益增加。这表明，推测解码的演进不仅是算法层面的，更是与系统级优化协同演进的结果。实现推测解码的峰值性能需要一种整体方法，不仅要考虑数学公式，还要考虑它如何与底层硬件、内存访问模式和软件栈交互。这预示着未来的进展将越来越多地来自机器学习算法、系统和硬件设计交叉领域的跨学科研究。

**表1: 推测解码技术演进**

| 年份 | 技术/论文 | 主要提出者 | 主要贡献/创新 | 加速比(约) | 相关片段ID |
| :---- | :---- | :---- | :---- | :---- | :---- |
| 2018 | 块级解码 | Stern 等人 | 使用FFN头部进行并行草稿生成和验证的开创性方法。 | 不适用 | 4 |
| 2022 | 通过推测解码实现Transformer的快速推理 | Leviathan 等人，Chen 等人 | 正式引入推测解码；自回归模型的无损加速。 | 约2-3倍 | 1 |
| 2023 | 推测采样 | Leviathan 等人，Chen 等人 | 将SD扩展到各种采样方法的无损加速；使用现成的小型LM作为草稿模型。 | 不适用 | 4 |
| 2023 | SpecDec | Xia 等人 | 使用独立的非自回归Transformer作为草稿模型；放宽验证标准。 | 约5倍 | 4 |
| `2023-2024` | SpecInfer, Assistant Generation, PPD, SpecTr, StagedSpec, Medusa, Draft & Verify, BiLD, LLMCad, EAGLE | 各研究者 | 草稿生成和验证策略的多样化发展，包括自草稿生成和令牌树验证。 | 可变 | 4 |
| 2025 | 奖励引导推测解码 (RSD) | 不适用 | 引入过程奖励模型动态决定目标模型调用，优化计算成本/输出质量权衡。 | 不适用 | 15 |
| 2025 | SPIRe | 不适用 | 采用静态稀疏注意力、剪枝初始化和反馈记忆的硬件高效草稿模型；吞吐量提升超过100%。 | 吞吐量提升超过100% | 14 |
| 2025 | SPRINTER | 不适用 | 低复杂度验证器预测令牌接受度，顺序验证，减少并行目标LLM调用。 | 不适用 | 8 |
| 2025 | 推测扩散解码 (SpecDiff) | 不适用 | 使用离散扩散模型进行并行草稿生成和验证，实现更高加速。 | 相较于标准方法高达7.2倍，相较于现有SD方法高达1.75倍 | 3 |

## **4\. 推测解码的模式与变体**

推测解码范式可根据其草稿生成和验证方法进行广泛分类，当前研究正不断完善这两个方面。

### **4.1 草稿生成策略: 独立与自草稿生成方法**

草稿模型的选择显著影响加速效果，其核心驱动因素是推测准确性和草稿生成延迟 4。

#### **4.1.1 独立草稿模型**

* **概念**: 使用独立于目标LLM的模型进行草稿生成 4。  
* **微调草稿模型**:   
  * **机制**: 一种专门的非自回归Transformer，如SpecDec(Xia等人，2023年)中所用，专为高效准确的草稿生成而设计。需要从头开始训练 4。  
  * **优点**: 如果针对目标模型进行良好微调，可能实现更高的准确性。  
  * **缺点**: 需要额外的训练成本和精力。  
* **免调优草稿模型**:   
  * **机制**: 直接使用与目标LLM同系列的小型现成LLM(例如，用于T5-XXL的T5-small，用于Llama 3.1-70B的Llama 3.1-8B)。这些模型通常共享分词器和训练过程，从而在预测行为上具有内在的一致性 4。  
  * **示例**: 推测解码(Leviathan等人，2023年)、StagedSpec、SpS、SpecTr、REST、CS. Drafting、MCSD 4。  
  * **优点**: 无需额外训练，快速采用，良好初始对齐。  
  * **缺点**: 性能严重依赖于内在对齐和模型大小差异 11。

    “微调”和“免调优”草稿模型之间的区别 4揭示了一个根本性的权衡。微调提供了更高推测准确性的潜力，但会产生训练成本。免调优模型部署成本较低，但依赖于固有的架构/训练相似性。这反映了部署中的实际工程决策。因此，草稿模型的最佳选择并非普遍的“性能最佳”模型，而是针对特定用例在开发/训练成本、部署复杂性以及期望的推理加速和接受率之间取得“最佳权衡”的模型。这表明该领域已趋于成熟，解决方案正根据实际操作需求进行定制。

#### **4.1.2 自草稿生成机制**

* **概念**: 利用目标LLM自身进行高效草稿生成，无需外部模型，解决了在没有较小对应模型时面临的挑战 4。  
* **FFN头部/依赖头部**:   
  * **机制**: 在Transformer解码器之上引入额外的全连接前馈网络(FFN)头部(例如，块级解码、Medusa、EAGLE)以并行生成令牌 4。这些头部轻量级，适用于分布式推理。  
  * **示例**: Medusa(Cai等人，2024年)、EAGLE(Li等人，2024年) 4。  
* **提前退出/层跳过**:   
  * **机制**: 在目标LLM内部使用中间层或自适应跳过层进行草稿生成(例如，PPD、Self-Speculative) 4。  
* **掩码预测**:   
  * **机制**: 在输入提示中附加多个\`\`令牌以进行并行生成(Santilli等人，2023年)。Lookahead Decoding和PaSS通过将低质量草稿转换为n-gram或使用可学习的\[LA\]令牌来改进此方法 4。

    自草稿生成机制 4代表了推测解码发展中的一个重要步骤。虽然独立草稿模型引入了第二个模型及其开销，但自草稿生成试图从目标模型本身中提取推测能力。这类似于通过重用现有计算或模型组件来获得“免费”的草稿生成能力。这种趋势反映了在最小化与草稿模型相关的计算和操作开销方面的努力。如果目标模型能够有效地为自身生成草稿，它将简化部署，减少内存占用，并可能提供更好的对齐，从而推动效率的边界。

### **4.2 验证策略: 从贪婪解码到推测采样和令牌树**

验证过程决定了每步接受的令牌数量，直接影响加速效果 4。

#### **4.2.1 贪婪解码及其近似方法**

* **贪婪标准**: 仅当草稿令牌与目标LLM的top-1预测“完全匹配”时才接受该令牌。如果令牌验证失败，则由LLM的top-1预测替换，并丢弃所有后续草稿令牌 4。  
* **示例**: Blockwise、SpecDec、Parallel Decoding、PPD、SPEED、Self-Speculative、Lookahead Decoding 4。  
* **近似验证**:   
  * **机制**: 放宽严格匹配要求以接受更多草稿令牌(例如，SpecDec仅要求令牌在top-k候选之内；BiLD使用回滚标准) 4。  
  * **权衡**: 旨在提高令牌接受率，可能以略微偏离严格贪婪输出为代价，尽管研究材料强调标准推测解码的无损保证 5。

    从严格的贪婪验证到“近似验证”的演变 4揭示了一种微妙的权衡。虽然推测解码的核心原则保证了无损输出，但一些变体探索了轻微的放宽以提高接受率。这表明，对于某些应用而言，如果能带来显著的性能提升，那么在严格概率保真度上的微小、可控偏差可能是可接受的。这表明对LLM部署的实际需求有了更深入的理解。并非所有应用都需要绝对的、数值上完全相同的输出。对于许多应用而言，统计上等效或“足够好”的近似值，如果速度显著更快，则更受青睐，从而为推测解码变体开辟了新的设计空间。

#### **4.2.2 用于随机生成的推测采样**

* **概念**: 将推测解码扩展到支持各种采样方法(例如，top-k、nucleus采样)，同时不改变目标LLM的输出分布 4。这对于创意或多样化的文本生成至关重要。  
* **验证标准**: 涉及一个随机数r，并根据概率比min(1, qi(xei) / pi(xei))接受令牌。如果被拒绝，则从调整后的分布中重新采样令牌 4。该标准在理论上已被证明能保持相同的输出分布 4。  
* **示例**: 推测解码(Leviathan等人，2023年)、DistillSpec、Online Speculative、SpS、CS. Drafting、PaSS 4。  
* **近似推测采样**: 与贪婪解码类似，存在近似策略通过略微放宽标准来提高接受率 4。

#### **4.2.3 令牌树验证**

* **概念**: 允许目标LLM通过将多个草稿序列合并到令牌树中并使用专门设计的树注意力掩码来并行验证它们 4。这是一种更高级的并行化策略。  
* **示例**: SpecInfer(Miao等人，2024年)、StagedSpec、SpecTr、REST、Medusa、EAGLE 4。

#### **4.3 相关优化方法: 用于模型对齐的知识蒸馏**

* **对齐的重要性**: 推测准确性受草稿模型与目标LLM之间行为相似性的显著影响 4。  
* **序列级知识蒸馏(Seq-KD)**:   
  * **机制**: 在目标LLM生成的句子上训练草稿模型(例如，块级解码) 4。  
* **集体增强微调(Col-BT)**:   
  * **机制**: 在训练数据上微调多个小型LLM，并将其聚合输出用作草稿(Miao等人，2024年) 4。  
* **其他知识蒸馏策略**: DistillSpec和Online KD探索了其他知识蒸馏策略，以解决采样方法导致的性能下降问题，并动态地将草稿模型与目标LLM对齐 4。  
* **奖励引导推测解码(RSD)**: 一种新颖的框架，它引入了一个过程奖励模型来评估中间解码步骤，并动态决定是否调用目标模型，从而优化计算成本与输出质量之间的权衡 15。

  知识蒸馏(KD)和奖励引导推测解码(RSD)的引入 4表明，研究已超越简单地选择一个静态草稿模型。这些方法旨在通过实时反馈来动态改进草稿模型的对齐或自适应地控制验证过程。这标志着从固定推测过程向更智能、自适应过程的转变。这种趋势表明，未来的推测解码实现将更加鲁棒和高效，能够根据实际情况动态调整其行为，以最大限度地提高接受率或优化资源利用。这使得推测解码更接近于一个“智能”推理系统，而非一个固定的算法。

**表2: 推测解码模式与变体比较**

| 类别 | 子类别 | 机制/方法 | 优势 | 劣势/考量 | 示例 / 相关片段ID |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **草稿生成策略** | **独立草稿生成** | 使用一个独立的、较小的模型生成候选令牌。 | 草稿模型设计灵活性高；可独立优化。 | 需要管理两个模型；若未仔细训练/选择，可能出现不对齐。 | 4 |
|    | 微调草稿模型 | 专门的非自回归Transformer，从头训练。 | 若微调得当，准确性高。 | 训练成本和精力高。 | SpecDec (Xia et al., 2023\) 4 |
|    | 免调优草稿模型 | 同系列中较小的现成LM。 | 无需额外训练；快速采用；内在对齐。 | 性能取决于内在对齐和大小差异。 | Llama 3.1-8B for Llama 3.1-70B 5；推测解码 (Leviathan et al., 2023\) 4 |
|    | **自草稿生成** | 利用目标LLM自身组件进行草稿生成。 | 无需外部模型；简化部署；可能对齐更好。 | 可能增加目标模型开销；设计复杂。 | 4 |
|    | FFN头部/依赖头部 | 在Transformer解码器之上引入额外FFN头部进行并行生成。 | 轻量级；适用于分布式推理。 | 需要架构修改。 | 块级解码 (Stern et al., 2018), Medusa, EAGLE 4 |
|    | 提前退出/层跳过 | 在目标LLM内部使用中间层或自适应跳过层。 | 重用现有模型组件。 | 确定最佳退出点的复杂性。 | PPD, Self-Speculative 4 |
|    | 掩码预测 | 附加\`\`令牌进行并行生成；通过n-gram或可学习令牌改进。 | 在单个模型内并行化的新颖方式。 | 未改进时可能生成低质量草稿。 | Lookahead Decoding, PaSS 4 |
| **验证策略** | **贪婪解码** | 仅当令牌与目标LLM的top-1预测完全匹配时才接受。 | 保证精确的贪婪输出。 | 严格性可能导致接受率较低。 | Blockwise, SpecDec, Parallel Decoding 4 |
|    | 近似验证 | 放宽严格匹配(例如，top-k候选)。 | 接受率更高。 | 可能略微偏离严格贪婪输出(但核心SD是无损的)。 | SpecDec (Xia et al., 2023), BiLD 4 |
|    | **推测采样** | 将SD扩展到随机采样方法；使用概率比进行接受。 | 保持采样生成输出分布一致。 | 验证逻辑更复杂。 | 推测解码 (Leviathan et al., 2023), DistillSpec 4 |
|    | 令牌树验证 | 通过合并到令牌树中验证多个草稿序列。 | 高级并行化；可探索多条路径。 | 令牌树构建和注意力掩码复杂性增加。 | SpecInfer, Medusa, EAGLE 4 |
| **相关优化** | 知识蒸馏 (KD) | 训练草稿模型以与目标LLM行为对齐。 | 提高推测准确性和接受率。 | 增加训练开销。 | Seq-KD, Col-BT, DistillSpec, Online KD 4 |
|    | 奖励引导推测解码 (RSD) | 使用过程奖励模型动态决定目标模型调用。 | 优化成本-质量权衡；对高奖励输出进行受控偏置。 | 增加奖励模型训练和推理的复杂性。 | RSD 15 |

## **5\. 性能分析与关键指标**

### **5.1 加速比与延迟改进(TTFT, TPOT)**

推测解码的有效性主要通过端到端延迟的减少和由此产生的加速比来衡量 5。加速比通过比较未启用推测解码与启用推测解码时的延迟来计算 12。LLM延迟包含两部分: 首令牌时间(Time to First Token, TTFT)和每输出令牌时间(Time Per Output Token, TPOT)。推测解码主要通过在每次昂贵的前向传播中获得更多令牌来优化TPOT 5。

**实证结果**: 

* **一般情况**: 推测解码在推理时间上显示出约2-3倍的改进 1。  
* **特定基准测试(AMD MI300X GPU与vLLM)**:   
  * Llama 3.1-70B与较小的草稿模型(Llama 3.1-8B、Llama 3.2-3B、Llama 3.2-1B)结合时，实现了超过2.0倍的加速，峰值达到 **2.31倍**(使用Llama 3.2-1B草稿模型) 12。  
  * Llama 3.1-405B实现了超过2.0倍的加速，峰值达到 **2.19倍**(使用Llama 3.1-8B草稿模型) 12。  
  * 对于更长的输入序列(例如32768个令牌)，加速比增加到 **2.98倍** 12。  
* **高级变体**: 推测扩散解码(Speculative Diffusion Decoding, SpecDiff)相对于标准生成方法展示了高达 **7.2倍的加速**，相对于现有推测解码方法则有 **1.75倍的加速** 3。SPIRe显示其吞吐量比现有草稿模型高出  
  **100%以上**，比稀疏自推测高出35%以上 14。

研究材料显示，加速比各不相同(通常为2-3倍，特定硬件上为2.31倍，高级方法高达7.2倍)，并强调加速比随输入序列长度的增加而提高 12。这表明加速并非一个固定乘数，而是高度依赖于模型、硬件、工作负载和具体的推测解码变体。因此，仅仅说明“推测解码提供X倍加速”是不够的。要获得细致的理解，需要考虑具体的部署环境，包括目标模型、草稿模型、硬件以及典型的输入/输出长度。这意味着基准测试和仔细调优对于实现推测解码的全部效益至关重要。

### **5.2 令牌接受率: 定义、测量与影响**

* **定义**: 接受率是已接受令牌与总草稿令牌的比率 10。它是衡量推测解码过程有效性的关键指标 10。  
* **对性能的影响**:   
  * **高接受率**: 表明草稿模型能很好地预测目标模型的输出，从而在每次验证步骤中生成更多令牌，提高加速比 10。这减少了两个模型的迭代次数 11。  
  * **低接受率**: 意味着频繁的错误推测，导致效率低下。目标模型最终会自行生成大部分响应，同时承担调用草稿模型的开销，可能损害性能 10。  
* **影响接受率的因素**:   
  * **草稿模型对齐**: 关键取决于草稿模型与目标模型令牌分布的匹配程度 11。  
  * **大小差异**: 草稿模型与目标模型之间显著的大小差异通常会导致较低的接受率 11。  
* **测量**: 可通过性能指标或计算器进行估算 11。

研究材料反复将加速比与接受率关联，并将接受率与草稿模型的质量/对齐程度关联 10。此外，研究材料 7指出，草稿模型的

**延迟** 至关重要，而其 **语言建模能力** 与推测解码的性能并不强相关。这造成了一个矛盾: 速度是关键，但准确性(接受率)也同样重要，而准确性通常意味着一个更强大的模型。这表明草稿模型选择存在一个“金发姑娘区”(Goldilocks Zone): 它必须足够快，以避免成为瓶颈(低延迟)，并且足够准确，以实现高接受率。仅仅小而快是不够的，如果它是一个糟糕的预测器。这推动了知识蒸馏 4和硬件高效设计 7等方法的研究，这些方法旨在在不牺牲速度的情况下提高准确性，甚至以非常浅层的模型实现高准确性 14。

### **5.3 影响性能的因素: 草稿模型选择、批处理大小、上下文长度、量化**

* **草稿模型选择**:   
  * **延迟**: 推测解码的性能在很大程度上取决于草稿模型的延迟 7。  
  * **能力**: 草稿模型的语言建模能力与推测解码的性能增益并不强相关 7。这意味着一个非常小、速度快的模型，如果能很好地对齐进行预测，也能发挥作用。  
  * **硬件效率**: 探索硬件高效草稿模型的新设计空间可以显著提高吞吐量 7。  
* **批处理大小**:   
  * **小批处理大小**: 推测解码在减少小批处理大小的延迟方面有充分的文献记载 5。在这种设置下，由于权重获取是瓶颈，因此更偏好小型草稿模型 14。  
  * **大批处理大小**: 如果上下文足够长且草稿模型的KV缓存稀疏，则可以加速大批处理大小的解码 14。然而，由于两个模型共享GPU资源，推测解码可能会限制高批处理大小的吞吐量 5。  
* **上下文长度**: 加速比随输入序列长度的增加而提高 12。推测解码在这些场景中更有效 12。  
* **量化**: 对草稿模型启用FP8量化可以进一步提升性能，即使对于较小的草稿模型也是如此 10。  
* **数据集**: 在使用不同数据集(例如，随机数据集与ShareGPT数据集)时，加速比没有显著差异 12。

**表3: 推测解码的经验性能基准**

| 目标LLM | 草稿模型 | 硬件 | 加速比(延迟降低) | 关键因素/上下文 | 相关片段ID |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.1-70B | Llama 3.1-8B, Llama 3.2-3B, Llama 3.2-1B | 单个AMD MI300X GPU | \&gt;2.0倍(Llama 3.2-1B峰值2.31倍) | 端到端延迟，vLLM框架 | 12 |
| Llama 3.1-405B | Llama 3.1-8B, Llama 3.2-3B, Llama 3.2-1B | 4个AMD MI300X GPU | \&gt;2.0倍(Llama 3.1-8B峰值2.19倍) | 端到端延迟，vLLM框架 | 12 |
| 通用LLM | 不适用 | 不适用 | 约2-3倍 | 原始论文，翻译/摘要任务 | 1 |
| 任意LLM | SPIRe(草稿模型) | 不适用 | 吞吐量比现有草稿模型高100%以上；比稀疏自推测高35%以上 | 针对吞吐量优化，大批处理大小，长上下文 | 14 |
| 任意LLM | 推测扩散解码 (SpecDiff) | 不适用 | 比标准生成方法高达7.2倍；比现有SD方法高达1.75倍 | 使用离散扩散模型进行草稿生成 | 3 |
| Llama 3.1-70B | 不适用 | 单个AMD MI300X GPU | 2.98倍 | 更长的输入序列(32768个令牌) | 12 |
| Arctic Inference | 不适用 | 不适用 | 对于开放式对话(ShareGPT)和代码生成(HumanEval)快2.8倍；比vLLM中最佳开源替代方案快1.8倍 | 优化后的推测代码路径，特定工作负载 | 10 |

## **6\. 优势、局限性与挑战**

### **6.1 主要优势与理想用例**

* **显著的延迟降低**: 推测解码最突出的优势是直接解决了自回归解码的顺序瓶颈 1。  
* **成本效益**: 通过在相同硬件上实现更快的生成，推测解码减少了服务所需的机器数量，从而降低了能源成本 1。  
* **保持输出质量**: 通过拒绝采样机制，推测解码保证了与目标模型相同的输出分布，确保质量不受影响 1。  
* **并行令牌计算**: 通过并行计算多个令牌，提高了并发性，进而改善了GPU利用率 1。  
* **理想用例**:   
  * **性能敏感应用**: 实时响应至关重要 2。  
  * **长输出**: 推测解码显著提升长序列生成的性能 11。  
  * **大型模型和小批处理大小**: 在大型模型受GPU内存限制的情况下，尤其适用于满足延迟服务水平协议(SLA)的场景 5。  
  * **代码生成**: 一个典型例子，需要大型、强大的模型生成长段代码，且延迟至关重要 5。  
  * **Google搜索中的AI概览**: 一个实际应用案例，证实了其有效性 1。

### **6.2 吞吐量权衡与资源共享考量**

推测解码在提高延迟的同时，可能限制模型服务器的最大吞吐量，尤其是在大批处理大小的情况下 5。这是因为两个模型共享相同的GPU资源 5。吞吐量的降低可能导致每令牌推理成本的增加，因为可能需要更多硬件来处理相同的流量 5。此外，两个模型共享VRAM但维护独立的KV缓存，这可能影响资源利用率 5。

研究材料呈现了一个明确的权衡: 推测解码在延迟方面表现出色，尤其是在小批处理大小下，但可能 **降低** 大批处理大小时的最大吞吐量 5。这对于部署场景而言是一个关键区别。延迟关乎单个用户的响应速度；吞吐量则关乎同时服务多个用户的能力。这意味着推测解码并非LLM服务所有挑战的万能药。其适用性在很大程度上取决于主要的优化目标。对于交互式应用(如聊天机器人、代码助手)，延迟至关重要，推测解码非常适用。而对于高吞吐量、批处理导向的处理，其他优化技术可能更合适，或者推测解码需要专门针对吞吐量进行优化(例如SPIRe 14)。这凸显了系统设计和工作负载分析的必要性。

### **6.3 编排复杂性与适用性限制**

* **编排开销**: 草稿模型与目标模型之间的协调循环(生成、发送、验证、循环)引入了编排复杂性。“这个循环中可能出现很多问题” 5。  
* **草稿模型选择的关键性**: 选择合适的草稿模型至关重要。一个糟糕的草稿模型(接受率低)可能抵消效益，甚至损害性能 11。  
* **硬件要求**: 尽管推测解码总体上降低了成本，但它仍然需要足够的GPU内存来同时承载两个模型，特别是对于大型目标LLM。  
* **特定场景下的有限效用**:   
  * **长输入短输出**: 可能导致效率低下，甚至产生负时间节省 11。  
  * **模型大小差异大**: 可能导致接受率降低 11。  
  * **并非所有开源框架都已优化**: 例如，vLLM指出推测解码“尚未优化，通常无法在所有提示数据集或采样参数下降低令牌间延迟” 13。  
  * **不兼容性**: 目前，vLLM中的推测解码与流水线并行不兼容 13。

除了算法的优雅性之外，研究材料还揭示了显著的工程挑战: 编排复杂性 5、草稿模型的谨慎选择 11以及特定的框架限制 13。这表明，尽管概念强大，但其在实践中的部署需要复杂的系统级集成和调优。推测解码在生产环境中的成功，不仅取决于理论上的加速，还取决于服务框架和工程师管理双模型设置、优化资源共享以及处理令牌接受动态性的能力。这预示着对健壮的开源实现和专业工具的持续需求，以使推测解码更广泛地可用并实现高性能。

## **7\. 开源实现与行业应用**

### **7.1 主要框架概览**

推测解码已在行业中得到广泛采用，并有许多有效的应用 1。

* **vLLM**: 一个流行的开源LLM服务系统，支持推测解码以提高服务效率。它提供了各种推测算法和优化技术，包括FP8量化 12。  
  * **特性**: 支持使用草稿模型、n-gram、MLP推测器和基于EAGLE的草稿模型进行推测 13。  
  * **当前状态**: 截至最新更新，vLLM中的推测解码在所有场景下“尚未优化”，正在进行工作以改善令牌间延迟的降低 13。目前，推测模型需要以非张量并行方式运行 13。  
* **Arctic Inference**: Snowflake为vLLM提供的增强型推测解码解决方案，声称在开放式对话和代码生成方面快2.8倍，比vLLM中其他开源替代方案快1.8倍 10。它提供了用于轻量级草稿模型的训练流水线和优化的推测代码路径 10。  
* **SambaStudio**: 允许用户使用现有模型创建推测解码对，以提高性能和效率，强调了草稿模型对齐的重要性 11。  
* **Google产品**: 推测解码已应用于多个Google产品，包括Google搜索中的AI概览，展示了显著的加速效果 1。

vLLM、Arctic Inference和SambaStudio的提及 10表明推测解码已从学术论文走向实际可部署的系统。这些框架积极致力于优化和集成推测解码的事实，表明了其被认可的价值以及使其实现生产就绪和易于访问的持续努力。开源生态系统的健康状况对于推测解码的广泛采用和进一步发展至关重要。随着这些框架的成熟并解决当前的限制(例如，vLLM的优化工作 13)，推测解码将成为LLM服务基础设施中更加标准和可靠的组件。

### **7.2 实际部署考量**

* **GPU利用率**: 推测解码旨在通过并行化令牌验证来提高GPU利用率，但在批处理时需要仔细管理 5。  
* **草稿模型对齐**: 为了获得最佳结果，草稿模型必须与目标模型的令牌分布紧密对齐，特别是对于微调或自定义检查点 11。  
* **性能指标**: 组织在部署前应使用性能指标或计算器来估算接受率和潜在的时间节省 11。  
* **硬件加速**: 与低功耗硬件加速器的集成可以进一步降低能耗 2。  
* **统一框架**: 像MAX(Modular)这样的平台简化了部署工作流程，使推测解码更易于访问 2。

## **8\. 未来方向与研究展望**

### **8.1 应对推测解码的当前挑战**

* **推测准确性与草稿生成效率之间的权衡**: 扩大草稿模型规模可以提高准确性，但会降低效率。行为对齐是一种有前景的方法，但仍需进一步改进，例如优先考虑早期位置令牌的生成质量 4。  
* **批处理推理场景**: 作为LLM实时服务的关键技术，批处理推理带来了挑战，因为解码步骤因推测准确性不同而异，且随着批处理大小的增加，额外计算复杂性也随之增加 4。当前正在进行研究以解决此问题 14。  
* **编排与资源管理**: 需要持续努力优化模型之间复杂的协调，并确保高效的资源分配，特别是在分布式系统中 5。

所强调的挑战(准确性与效率的权衡、批处理推理、编排 4)都指向同一个潜在需求: 将推测解码扩展到处理更复杂的场景和更大的工作负载。最初的推测解码侧重于单流延迟。未来在于使其在大规模并发部署中变得健壮和高效。未来的研究可能会集中于更具适应性和智能的草稿模型选择、考虑可变接受率的动态批处理策略，以及与硬件和编译器优化的更紧密集成，以在大规模部署中释放更多性能增益。

### **8.2 与其他高级LLM技术的集成**

* **协同组合**: 推测解码具有与其他高级技术集成的潜力，例如对比解码、vLLM的内部优化(例如，FlashAttention、连续批处理)以及非自回归生成方法 4。  
* **编译器与硬件优化**: 系统级考量，包括内核设计、硬件加速和批处理优化，正在推动创新，影响算法选择和实际性能 6。

关于将推测解码与其他技术集成 4以及强调编译器/硬件优化 6的讨论，表明了LLM推理的未来优化不仅停留在算法层面，而是贯穿整个软件/硬件堆栈。这预示着从孤立优化转向整体协同设计的系统。LLM的最终性能增益很可能来自于一个“统一优化堆栈”，其中推测解码是众多组件之一，所有组件协同工作。这需要机器学习研究人员、系统工程师和硬件设计人员之间的深度协作，从而形成高度专业化和高效的推理流水线。

### **8.3 新兴方法与多模态应用**

* **奖励引导推测解码(RSD)**: 一个有前景的方向，旨在引入受控偏置并动态优化计算成本与输出质量之间的权衡 15。  
* **推测扩散模型(SpecDiff)**: 提出使用离散扩散模型生成草稿序列，允许草稿生成和验证步骤的并行化，从而实现显著加速 3。这代表了一种新颖的草稿生成方法。  
* **多模态推理**: 将推测解码应用于文本以外的领域，如图像合成、文本到语音和视频生成，是一个引人入胜的方向 1。  
* **推荐系统**: 在实时推荐系统中的潜在应用 6。

提及多模态应用(图像、语音、视频生成)和推荐系统 1显著拓宽了推测解码的范围。这意味着“推测生成和并行验证”的核心原则不仅限于文本中的离散令牌，还可以推广到任何顺序生成任务。这种扩展表明，推测解码正在演变为一种基础的、领域无关的自回归模型优化技术，适用于各种AI模态。这可能在以前受顺序生成瓶颈限制的领域中解锁实时能力。

## **结论**

推测解码对大语言模型推理产生了变革性影响，实现了更快、更具成本效益且保持质量的生成。其演进历程从基础概念发展到多样化的模式，突显了草稿模型和接受率的关键作用，以及在系统级优化方面的持续努力。

未来，该领域将继续充满活力，研究将致力于解决现有局限性，并积极探索与其他高级技术的集成以及向多模态AI应用的扩展。推测解码的持续发展，预示着LLM推理效率的进一步飞跃，使其在更广泛的实际应用中发挥更大潜力。

## **参考文献**

* 7  
  [https://arxiv.org/abs/2402.01528](https://arxiv.org/abs/2402.01528)  
* 4  
  [https://arxiv.org/abs/2401.07851](https://arxiv.org/abs/2401.07851)  
* 15  
  [https://arxiv.org/html/2501.19324v1](https://arxiv.org/html/2501.19324v1)  
* 14  
  [https://arxiv.org/pdf/2504.06419](https://arxiv.org/pdf/2504.06419)?  
* 8  
  [https://arxiv.org/html/2502.04557v1](https://arxiv.org/html/2502.04557v1)  
* 9  
  [https://arxiv.org/html/2402.01528v4](https://arxiv.org/html/2402.01528v4)  
* 12  
  [https://rocm.blogs.amd.com/software-tools-optimization/speculative-decoding---deep-dive/README.html](https://rocm.blogs.amd.com/software-tools-optimization/speculative-decoding---deep-dive/README.html)  
* 1  
  [https://research.google.com/blog/looking-back-at-speculative-decoding/](https://research.google.com/blog/looking-back-at-speculative-decoding/?authuser=1)  
* 2  
  [https://www.modular.com/ai-resources/how-speculative-decoding-speeds-up-llm-inference](https://www.modular.com/ai-resources/how-speculative-decoding-speeds-up-llm-inference)  
* 5  
  [https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/](https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/)  
* 11  
  [https://docs.sambanova.ai/sambastudio/latest/spec-decoding.html](https://docs.sambanova.ai/sambastudio/latest/spec-decoding.html)  
* 1  
  [https://research.google.com/blog/looking-back-at-speculative-decoding/](https://research.google.com/blog/looking-back-at-speculative-decoding/?authuser=1)  
* 10  
  [https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/](https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/)  
* 13  
  [https://docs.vllm.ai/en/latest/features/spec\_decode.html](https://docs.vllm.ai/en/latest/features/spec_decode.html)  
* 3  
  [https://arxiv.org/html/2408.05636v4](https://arxiv.org/html/2408.05636v4)  
* 6  
  [https://arxiv.org/html/2502.19732v1](https://arxiv.org/html/2502.19732v1)  
* 1  
  [https://research.google.com/blog/looking-back-at-speculative-decoding/](https://research.google.com/blog/looking-back-at-speculative-decoding/?authuser=1)  
* 1  
  [https://research.google.com/blog/looking-back-at-speculative-decoding/](https://research.google.com/blog/looking-back-at-speculative-decoding/?authuser=1)  
* 5  
  [https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/](https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/)  
* 12  
  [https://rocm.blogs.amd.com/software-tools-optimization/speculative-decoding---deep-dive/README.html](https://rocm.blogs.amd.com/software-tools-optimization/speculative-decoding---deep-dive/README.html)  
* 5  
  [https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/](https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/)  
* 4  
  [https://arxiv.org/abs/2401.07851](https://arxiv.org/abs/2401.07851)  
* 1  
  [https://research.google.com/blog/looking-back-at-speculative-decoding/](https://research.google.com/blog/looking-back-at-speculative-decoding/?authuser=1)

#### **Works cited**

1. Looking back at speculative decoding \- Google Research, accessed June 26, 2025, [https://research.google/blog/looking-back-at-speculative-decoding/](https://research.google/blog/looking-back-at-speculative-decoding/)  
2. How Speculative Decoding Speeds Up LLM Inference \- AI Resources \- Modular, accessed June 26, 2025, [https://www.modular.com/ai-resources/how-speculative-decoding-speeds-up-llm-inference](https://www.modular.com/ai-resources/how-speculative-decoding-speeds-up-llm-inference)  
3. Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion \- arXiv, accessed June 26, 2025, [https://arxiv.org/html/2408.05636v4](https://arxiv.org/html/2408.05636v4)  
4. Unlocking Efficiency in Large Language Model Inference: A ..., accessed June 26, 2025, [https://arxiv.org/abs/2401.07851](https://arxiv.org/abs/2401.07851)  
5. A quick introduction to speculative decoding | Baseten Blog, accessed June 26, 2025, [https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/](https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/)  
6. Speculative Decoding and Beyond: An In-Depth Review of Techniques \- arXiv, accessed June 26, 2025, [https://arxiv.org/html/2502.19732v1](https://arxiv.org/html/2502.19732v1)  
7. \[2402.01528\] Decoding Speculative Decoding \- arXiv, accessed June 26, 2025, [https://arxiv.org/abs/2402.01528](https://arxiv.org/abs/2402.01528)  
8. Speeding up Speculative Decoding via Approximate Verification \- arXiv, accessed June 26, 2025, [https://arxiv.org/html/2502.04557v1](https://arxiv.org/html/2502.04557v1)  
9. Decoding Speculative Decoding \- arXiv, accessed June 26, 2025, [https://arxiv.org/html/2402.01528v4](https://arxiv.org/html/2402.01528v4)  
10. Fastest Speculative Decoding in vLLM with Arctic Inference and Arctic Training \- Snowflake, accessed June 26, 2025, [https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/](https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/)  
11. Speculative decoding \- SambaNova Documentation, accessed June 26, 2025, [https://docs.sambanova.ai/sambastudio/latest/spec-decoding.html](https://docs.sambanova.ai/sambastudio/latest/spec-decoding.html)  
12. Speculative Decoding \- Deep Dive — ROCm Blogs, accessed June 26, 2025, [https://rocm.blogs.amd.com/software-tools-optimization/speculative-decoding---deep-dive/README.html](https://rocm.blogs.amd.com/software-tools-optimization/speculative-decoding---deep-dive/README.html)  
13. Speculative Decoding \- vLLM, accessed June 26, 2025, [https://docs.vllm.ai/en/latest/features/spec\_decode.html](https://docs.vllm.ai/en/latest/features/spec_decode.html)  
14. SPIRe: Boosting LLM Inference Throughput with Speculative Decoding \- arXiv, accessed June 26, 2025, [https://arxiv.org/pdf/2504.06419?](https://arxiv.org/pdf/2504.06419)  
15. Reward-Guided Speculative Decoding for Efficient LLM Reasoning \- arXiv, accessed June 26, 2025, [https://arxiv.org/html/2501.19324v1](https://arxiv.org/html/2501.19324v1)