---
id: batching-strategies
sidebar_position: 2
title: 批处理策略
---

## LLM批处理技术详解

大语言模型(LLM)的**批处理(Batch Processing)**技术是提升推理吞吐量、降低成本的关键手段。通过将多个请求合并一起处理，可以大幅提高GPU利用率、减少空闲时间，从而显著加快模型响应并节省开销。例如，对于LLaMA2-70B模型，批处理将吞吐量从每秒约200个token提升到了1500个token，同时节省了约40%的成本。许多公司已经通过批处理获得了实际收益:Zendesk通过批处理技术将响应速度提升了62%，每年节省数百万美元。可见，无论是ML工程师还是系统架构师，都应掌握LLM批处理的原理和实现，以 **更高效、更低成本** 地部署大型模型。

### 批处理策略概述

LLM推理中的批处理策略主要包括**静态批处理**、**动态批处理**和**连续批处理**三种形式，它们适用于不同的工作负载场景。

* **静态批处理(Static Batching)**:预先固定批次大小，累积足够数量的请求后一次性处理。这种方式确保每次计算都充分利用GPU，但如果请求到达不均匀，会导致请求等待时间较长。静态批处理适合**负载稳定且对延迟不敏感**的场景(例如离线的大批量文档处理)，不太适合需要实时响应的应用。

* **动态批处理(Dynamic Batching)**:实时收集并打包请求，在**达到设定的最大批大小**或**等待超时时间上限**时触发模型推理。这种策略兼顾吞吐和延迟，在请求流量高时尽可能组成大批次提高效率，流量低时则按超时阈值及时处理部分批次，避免个别请求长时间滞留队列。动态批处理非常适合**交互式实时**场景(如聊天机器人、在线问答)，能够在保证延迟上线的同时提升总体吞吐。

* **连续批处理(Continuous Batching)**:也称**连续动态批处理**，这是针对LLM特点的高级批处理技术。与按请求批次的动态批处理不同，连续批处理在**token级别**调度模型推理:模型按序逐层计算，每层对当前所有未完成请求各生成下一个token，然后循环处理，**不必等一个批次内最长响应完成后再处理下一个批次**。这样，当某请求完成其生成后，新的请求可以及时加入计算 “队列”，GPU无需等待最长响应的尾部时间，从而**消除批次内部的闲置**。连续批处理极大提高了GPU利用率和吞吐，特别适合输出长度差异大的LLM场景。例如，Anthropic针对Claude 3模型采用连续批处理，将吞吐量从每秒50个token提高到450个，同时平均延迟从2.5秒降至0.8秒，GPU成本降低40%。目前HuggingFace的文本生成Inference服务器(TGI)和vLLM等框架已经支持连续批处理(TensorRT-LLM中称为“飞行批处理”)。连续批处理在共享服务环境下显示出惊人的效果，有研究表明可带来10–20倍的吞吐提升甚至超过20倍。

*动态批处理通过在批次装满或超时阈值到达时立即执行模型推理，避免了静态批处理必须等待批次填满所带来的额外延迟。上图演示了当流量较低时，动态批处理相比静态策略能更快启动处理请求，从而降低平均等待时间，同时在高流量下仍可靠累积请求获得与静态批处理相当的吞吐量。*

总的来说，静态批处理追求**极限吞吐**适合批处理作业，动态批处理平衡**吞吐和延迟**适合实时服务，而连续批处理进一步提升**GPU利用率**以追求更高吞吐、特别适用于LLM在线服务。在实际系统中可以视需求选择或组合这些策略。

### 动态批处理的实现原理

动态批处理是LLM在线推理中应用最广泛的策略。其核心思想是引入一个**调度队列和批处理器**，智能地将并发请求打包成批。下面通过代码示例讲解其工作原理
```python
class DynamicBatcher:
    def __init__(self, max_batch_size, timeout_ms):
        self.max_batch_size = max_batch_size
        self.timeout_ms = timeout_ms
        self.queue = Queue()
        
    async def add_request(self, request):
        future = Future()
        # 将请求和Future放入队列，Future用于异步获取结果
        await self.queue.put((request, future))
        return await future  # 等待批处理完成后返回结果
        
    async def batch_processor(self):
        while True:
            batch = []
            start_time = time.time()
            # 收集请求直到达到最大批大小或超时
            while len(batch) < self.max_batch_size:
                try:
                    request, future = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=self.timeout_ms/1000
                    )
                    batch.append((request, future))
                except TimeoutError:
                    break
            if batch:
                # 处理批次:一次性将所有请求送入模型
                results = await process_batch([req for req, _ in batch])
                # 将结果填入各自的Future，返回给最初的请求调用方
                for (_, future), result in zip(batch, results):
                    future.set_result(result)
```

上述`DynamicBatcher`利用异步队列和定时等待机制实现了动态批处理的调度流程:

* **请求入队**:每当有新请求来到，通过`add_request`方法将请求包装成包含`Future`的任务对入队列等待处理。`Future`对象用于承诺稍后返回结果，使得请求可以异步挂起等待。

* **批次组装**:后台的`batch_processor`协程持续运行，从队列中提取请求构建当前批次。它在循环中收集请求，直到批次大小达到预设`max_batch_size`，或者等待超过`timeout_ms`毫秒后不再有新请求到来。从实现上使用了`asyncio.wait_for`设置超时来跳出等待循环。这样就实现了“**最大批大小或超时**”两个条件触发批处理。

* **批量推理**:一旦满足触发条件，`batch_processor`会调用`process_batch`函数对组装好的请求列表进行批量推理。这通常是调用底层的模型推理接口(例如PyTorch或TensorRT)一次性处理整个batch的数据张量。

* **结果回传**:批量推理返回各请求的结果列表后，代码通过之前保存的`Future`对象将对应结果赋值，唤醒等待的请求任务。这样每个请求都异步地收到了自己的推理结果，整个批次处理完成，`batch_processor`随后继续下一轮循环等待后续请求。

这种实现方式确保了在并发请求高的时候，尽可能将它们**打包为接近最大批大小**的一批，以充分发挥GPU的并行计算能力；而当请求稀疏时，也能在合理延迟内及时执行已经积攒的请求，避免过度等待。NVIDIA Triton推理服务器就提供了类似的动态批处理调度机制，允许用户为每个模型配置最大批大小和队列等待时间等参数。

**动态批处理的效果**是显著的:在理想情况下，如果没有动态批处理，5个请求串行执行耗时约`5X`毫秒；引入动态批处理并将它们打包，可在约`3X`毫秒内全部完成，**总吞吐提高接近67%**，同时由于更少的循环轮次也**降低了平均延迟**。当然，批处理过大也会遇到**边际效益递减**和开销增加的问题，需要合理权衡批大小和延迟要求。

下面将详细讨论提升动态批处理效率的一系列**优化策略**，包括批大小和超时的自适应调节、优先级处理、资源监控与负载均衡等。

#### 自适应批大小

选择合适的批大小对于平衡吞吐量和延迟至关重要。在静态配置中，批大小往往被视为固定超参数，但动态场景下我们可以**根据实时负载和性能目标动态调整批大小**。自适应批大小策略通过监控系统状态(例如当前队列长度、GPU利用率、近期请求延迟等)来增减批次规模，从而适应变化的工作负载:

```python
def adjust_batch_size(current_load, current_latency, latency_threshold):
    """
    根据系统负载和延迟SLA动态调整批大小。
    """
    if current_load > HIGH_LOAD_THRESHOLD:
        return increase_batch_size()      # 加大批次，提高吞吐
    elif current_latency > latency_threshold:
        return decrease_batch_size()      # 降低批次，减少延迟
    else:
        return current_batch_size
```

上面的伪代码展示了基本思路:当系统**负载高**时(比如队列长度或吞吐需求高于某阈值)，尝试增大批处理规模以提升整体吞吐；反之如果**请求延迟**高于设定的服务级别协议(SLA)上限，则减小批大小以降低每批等待时间，从而改善延迟。平时则保持当前批大小不变。一项最新研究表明，通过内存感知的批大小自适应控制，可以在确保延迟SLA的同时，将吞吐提升8%到28%，服务容量提升22%。

实现自适应批大小需要配合**监控指标**(见后文监控部分)持续观察系统状态，并选取适当的调整步长以避免批大小频繁抖动带来的开销。Anthropic团队曾使用**自定义的批调度器**，根据输入序列长度和可用GPU显存动态调批，使Claude模型的推理吞吐提升了37%、延迟下降28%。可见综合考虑**序列长度、显存占用**等因素调整批次上限，是取得最佳性能的关键。

#### 动态超时设置

除了批大小，自适应**等待超时**也是动态批处理优化的重要手段。超时时间决定了请求进入队列后最长等待多久就必须启动批处理。固定的超时阈值在不同流量条件下并非最优:

* 在**低负载**时，适当**延长**等待时间(例如从50ms增加到100ms)，可以攒到更多请求一起处理，提高每批GPU利用率。由于此时流量小，额外的几十毫秒等待对用户影响不明显，却能避免小批次造成的吞吐浪费。

* 在**高负载**时，考虑**缩短**等待时间，甚至可以接近0(如Triton支持配置0延迟立即调度)。因为高并发下很快就会积累满批请求，过长的等待只会徒增延迟。此时每批基本都会达到最大大小，主要目标是尽快处理以防止队列积压。

动态调整超时策略通常与批大小自适应结合使用:例如当系统检测到平均延迟增高时，同时**减小批大小和等待时间**来迅速清空队列；反之负载低且GPU利用率偏低时，可**略微增加等待时间**以获取更大批次。需要注意超时过短可能导致批处理频繁触发、小批次过多，过长则可能违背延迟要求。因此应根据历史请求到达率和延迟分布来制定一个动态调整规则，在**吞吐**和**响应延迟**之间取得平衡。

#### 优先级队列管理

在实际系统中，不是所有请求的紧急程度都相同。**高优先级**的交互请求(如实时对话)可能需要比批量后台任务(如日志分析)更低的延迟保障。为此，可以在动态批处理中引入**优先级队列**调度机制，将请求按优先级分类处理:

```python
class PriorityBatcher:
    def __init__(self):
        self.high_priority_queue = Queue()
        self.normal_queue = Queue()
        
    async def add_request(self, request, priority="normal"):
        if priority == "high":
            await self.high_priority_queue.put(request)
        else:
            await self.normal_queue.put(request)
```

如上，我们维护两个独立的队列，高优先级请求进入`high_priority_queue`，普通请求进入`normal_queue`。接下来在批处理循环中，可优先从高优先级队列取出请求进行批处理，当高优先级队列为空时再处理普通队列。这保证了紧急请求不会长时间淤积在大量普通请求之后。

具体实现上，可以对前述`batch_processor`进行改造，每轮从`high_priority_queue`中尽可能取满一个批次；如果高优先级请求不足批大小且队列已空，再从普通队列补充剩余名额组成混合批次。一些推理服务框架已支持**请求优先级**调度，例如Triton服务器允许为不同模型实例组设置优先级权重，从而在资源争用时优先满足高优先级组。

优先级批处理策略需要综合考虑:高优先级请求过多时可能导致普通请求**饥饿**(starvation)，因此可以在调度算法中加入适当的配额或老化机制，确保低优先级请求也能得到服务。此外，要注意**批效率**:如果高优先级流量非常低，始终单独处理它们会损失批处理带来的并行效率，可能需要在延迟允差内适当等待是否有普通请求可以合批以提高GPU利用率。

#### 资源利用优化

动态批处理的性能离不开对**系统资源**的高效利用。这里包括GPU显存、计算核心以及内存/缓存等。几项重要的资源优化策略如下:

* **GPU显存监控**:实时监测GPU显存占用情况，防止组建批次时超出显存限制导致OOM错误。批大小自适应调整时应以显存余量为约束，必要时降低批大小或 sequence 长度上限。使用NVIDIA的`nvidia-smi`或DCGM等工具可以跟踪显存使用。另外，可结合优化技术如混合精度、模型压缩来减少单样本显存占用，从而容纳更大批次。

* **计算资源调度**:充分利用GPU/CPU的计算能力，避免某些核心闲置和热点。对于多GPU服务器，可采用**并行实例**或模型并行等方式分摊负载。Triton支持在同一GPU上启用多个模型实例以提高并发度。如果单次推理无法利用满GPU算力(例如小模型或短序列)，可以通过**同时运行多个推理进程**或使用GPU流并行执行多个批次，提高总体吞吐率。需要根据模型特性调整并发度，使GPU利用率接近100%但不发生资源争抢。

* **内存使用平衡**:在多模型或多租户场景下，注意不同任务对GPU内存和主机内存的占用。可以对多个模型的批处理进行**统一调度**，避免同时出现内存峰值。例如当大模型A批处理中占满显存时，可暂停模型B的批处理，等A完成释放显存后再执行B，以避免互相争夺资源。对于CPU内存，也要定期检查队列积压导致的对象数量，防止内存泄漏或不足。

通过以上资源管理，动态批处理器才能在各种负载水平下**平稳高效**地运行，既避免资源浪费又防止资源瓶颈。正如经验所示，高效的LLM推理往往是**内存带宽受限**的，在确保不超出内存限制的前提下尽量增大批次，才能充分发挥GPU并行吞吐能力。

#### 超时控制

健壮的批处理系统需要处理各种超时情况，包括**请求级超时**和**批次级超时**，并配套相应的**降级策略**:

* **请求级超时**:针对单个请求设置的最大容忍等待/处理时间。如果某请求从进入系统开始等待时间超过阈值(例如用户侧要求的SLA)，系统可以选择提前返回超时错误或降级结果。这在动态批处理场景下相当于为每个请求设置了**最后期限**，批处理调度需考虑避免让请求等待超过其超时时间。例如实现上，可在`add_request`时记录请求的开始时间，在`batch_processor`组批时跳过已经超时的请求并标记失败，从而防止无限期挂起。

* **批处理超时**:即前述批处理调度的等待窗口阈值。这个超时主要是为了**权衡延迟和批大小**，避免等待过久。在实现时已经通过`asyncio.wait_for`等机制处理，一旦超过设定时间就应立即执行已有的部分批次。需要根据应用场景的**延迟敏感度**来配置合理的批次超时时间，通常在几十毫秒到几百毫秒量级。很多推理服务器默认允许0~200ms范围内调整，例如Triton中`max_queue_delay_microseconds`参数可设置等待上限。

* **降级策略**:当系统负载过高或出现异常情况时，必须有降级机制保证服务的可用性。常见的降级措施包括:

 * **转单请求处理**:如果批处理队列堆积严重且延迟变得不可接受，可以临时将部分请求绕过批处理器，直接单独推理迅速返回结果(牺牲部分吞吐换取及时响应)。
 * **简化模型**:在极端高负载或资源紧张时，切换到一个精度更低、速度更快的模型或缩短最大输出长度，以减少单次推理开销。
 * **请求丢弃/拒绝**:在达到系统极限时，主动拒绝新请求或丢弃低优先级请求，避免队列无界增长影响整体服务。通过负载监控触发限流策略，将压力降至可控范围。

降级策略在实现时需谨慎，确保只在必要时触发，并尽快恢复正常批处理。一种简单实现是在批处理出现异常(如模型崩溃、内存不足)时捕获异常，fallback到逐个请求处理:

```python
async def handle_batch_error(batch):
    """
    批处理错误的恢复机制:将批内请求逐一降级处理
    """
    try:
        results = await process_batch(batch)  # 正常批处理
    except Exception as e:
        results = []
        for request in batch:
            try:
                result = await process_single(request)  # 降级为单请求处理
                results.append(result)
            except Exception:
                results.append(error_response())       # 返回错误响应占位
    return results
```

通过以上超时与降级控制，系统可以在各种异常和高压场景下**保持稳定**:既满足大部分时间的高效批处理，又能在必要时牺牲部分性能保证服务不中断。

#### 负载均衡

当请求量持续攀升或单机资源吃紧时，需要考虑**横向扩展**和智能路由，即负载均衡策略，以进一步提高批处理系统的吞吐和可靠性:

* **多实例分发**:启动**多个模型实例**来并行处理请求。NVIDIA Triton允许在一块GPU上开启多个同模型实例，或在多块GPU上分别部署实例，通过配置`instance_group`来实现。在动态批处理上下，多个实例可以各自执行批调度，互不阻塞。例如有两个实例时，请求可以分流，两边各自动态打包批次，从而几乎翻倍整体吞吐。尤其当单实例批处理已达到模型batch上限或GPU上限时，新增实例是唯一提高吞吐的方法。需要注意的是实例数增加会瓜分硬件资源，要根据模型大小和GPU容量决定合理的实例数。

* **资源感知路由**:当系统由多台服务器或多GPU组成时，应设计**调度器**将请求分配给**最有空闲资源**的节点。例如维护各节点当前批队列长度、GPU利用率、显存占用等指标，新的请求优先路由至队列最短、资源最充裕的实例。这避免某些实例过载队列积压，而其他实例空闲。实现上可以使用**集中式队列**结合各实例心跳信息，或者像Kubernetes那样利用服务网格监控负载进行流量分配。资源感知的路由还能结合请求特征:例如长序列请求分配给拥有更多显存的节点，小请求集中到一个节点避免打乱其他节点批次等。

* **动态扩缩容**:结合容器编排或服务器无状态设计，实现根据负载自动增减实例的能力。当请求QPS长时间高于阈值时，自动启动新的模型服务实例(水平扩展)并加入负载均衡；当负载下降时，缩减多余实例节省资源。云服务环境下可以利用Kubernetes HPA(Horizontal Pod Autoscaler)或自定义脚本依据指标伸缩。动态扩缩容对批处理系统尤为重要:因为批处理提高了单实例吞吐，一旦流量超过单机能力，扩容是唯一出路，否则延迟将急剧升高。需要确保扩容过程平滑，如新实例启动前老实例不要过早卸载，避免服务中断。另外，引入更多实例后也需考虑批处理的**跨实例协调**(比如可选地在负载非常高时，允许不同实例间合并批次？一般不会这么做，主要是均匀分配请求即可)。

通过上述负载均衡措施，LLM推理服务可以从**单实例优化**提升到**多实例协同**:既能在常规情况下以最少资源运行，又能在高峰期迅速扩展满足请求需求，从而达到既高性能又高可用的目标。

### 最佳实践

综合以上技术点，在设计和部署LLM批处理系统时，建议遵循以下最佳实践
* **根据硬件能力设定批大小上限**:批大小受限于GPU显存和模型占用。比如80GB的A100显卡可支撑的batch远大于32GB的V100。首先根据模型大小和可用显存计算理论批次上限，然后酌情留有余量(预留一些内存给缓存和中间激活)。切忌一味追求超大批次，超过**64**往往收益递减且容易引发显存不足。

* **设置合理的批处理等待超时**:在满足应用延迟要求前提下尽量给批处理留出缓冲时间。典型地，在线服务将等待窗口设为50ms~200ms不等，具体取决于请求的到达率和用户容忍度。可以从较小值起步，根据监测到的平均批大小和延迟调整。对于毫秒级要求的场景，可以考虑直接使用连续批处理或多实例而将此值调低。

* **监控系统资源与性能**:部署批处理后，要持续监控**GPU利用率**、**显存使用**、**批处理队列长度**、**请求延迟分布**等指标。通过监控可以及时发现批大小不当(例如平均批大小远小于上限则可能窗口太短或流量不足，反之持续打满上限但延迟升高则说明可能需要扩容)。同时跟踪**错误率**和OOM事件日志，确保及时调整相关参数或修复问题。

* **逐步扩展和预热模型**:在上线批处理前，先对模型服务进行**预热**(见下文)，避免一上来真实请求遭遇模型加载或JIT开销导致超时。另外，启用批处理后应逐步增加负载并验证性能，找到**转折点**(例如批次过大会导致延迟陡增的点)并据此调整策略。也可以准备回退方案，在批处理模式不如预期时切换回单请求模式以保证可靠性。

* **结合领域需求调整**:不同应用对吞吐和延迟的侧重不同。**交互式场景**下，应优先保障低延迟，可牺牲一些吞吐(采用较小批次或连续批处理)；而**离线批处理**任务可允许较高单任务延迟换取整体效率(采用大批次、静态批处理)。根据具体场景选对策略，并非一味追求最大批大小就是最佳。

### 性能优化

在实现了基本的批处理功能后，还可以通过一些工程技巧对系统进行**性能优化**，进一步挖掘潜力
#### 预热策略

LLM模型首次运行时通常会有加载权重、编译图等开销，造成显著冷启动延迟。为避免将这部分延迟摊到真实请求上，可以在系统启动时进行**批处理器预热**:

```python
async def warmup_batcher(batcher):
    """
    系统启动时预热批处理器，提前加载模型与库
    """
    dummy_requests = generate_dummy_requests(n=batcher.max_batch_size)
    await batcher.add_request(dummy_requests[0])  # 单请求预热一次
    # 或者直接调用内部批处理函数处理一批虚拟请求
    await process_batch(dummy_requests)
```

上述示例通过构造虚拟请求集，在系统初始化时手动触发一次批处理。这会强制模型权重加载到GPU、JIT编译模型(如使用TorchScript或TensorRT时)以及分配所需的显存张量。预热后，后续真实请求的处理就能避开首次调用的高延迟。实践中，可以设计一个**定时预热**机制，比如服务空闲时定期发送一些“空请求”通过批处理路径，确保模型一直留在内存、保持“热身”状态。

值得注意的是，预热请求应**小心设计**:既要代表实际负载(如序列长度、数据类型)，又不能影响系统状态(可以在模型输出后立即丢弃结果)。通过预热，许多系统报告**首字节延迟**(TTFB)大幅下降，从而提升用户交互体验。

#### 内存管理

LLM推理往往会大量使用GPU显存和CPU内存，在长时间运行中需要注意**清理和复用**，防止内存膨胀影响性能:

* **释放未用显存**:对于使用PyTorch的服务，每次大批量推理后，可以显式调用`torch.cuda.empty_cache()`释放缓存的显存碎片，以供后续批次利用。但要谨慎频率，过于频繁清cache本身也有开销，一般在检测到OOM风险或大型任务后才执行。另外，如果使用TensorRT或静态图优化，尽量复用分配的显存池。

* **垃圾回收**:批处理过程中会产生许多临时对象(请求数据结构、输出结果等)，可以定期调用Python的`gc.collect()`主动触发垃圾回收，避免内存泄漏。特别是在循环处理时，确保队列中无用的对象能够及时销毁。

* **KV Cache管理**:对于自回归LLM，注意**Key-Value缓存**的增长。随着批中序列长度增加，KV缓存占用显存线性上升。可以采用**分段推理**(定长缓存大小)或定期清理长上下文的缓存来限制显存消耗。某些模型提供了合并KV或降低精度存储缓存的方法(如Grouped Query Attention将KV head数减少来省显存)，可结合使用以平衡长序列支持和批处理规模。

* **避免内存碎片**:长时间运行后GPU内存可能变得支离破碎，影响新batch的大张量申请。除了empty\_cache，也可以通过**模型重启**或**CUDA缓存分配器**等手段缓解。比如将模型每隔固定时间重置加载一次，或者使用NVIDIA的显存池化API确保大块显存重用。

通过良好的内存管理，可以防止批处理运行过程中因为内存问题导致的性能抖动或崩溃，保障服务长时间稳定运行。

### 异常处理

批处理系统的异常处理尤其重要，因为一次批处理中出错可能影响多个请求。必须实现健壮的错误捕获和恢复机制，典型包括
* **批内部异常隔离**:如前述`handle_batch_error`示例，尽量将批处理过程包裹在`try/except`中。如果某批次全部请求一起调用模型时发生异常(可能原因:模型推理函数内部报错、设备故障等)，则立即启动降级策略，对该批中的请求逐一单独处理或返回错误。这样可以避免因为一个批次的问题而**堵塞后续所有批处理**。同时要记录错误日志，包含异常类型、当前批请求详情，便于后续分析问题根源(例如是否某特定输入导致模型Bug)。

* **单请求异常处理**:即使批处理本身成功，也可能**某些请求的结果无效**(例如输入格式错误导致模型无法生成结果)。批处理框架应支持对每个请求结果进行检查，标记失败的请求并返回预定义的错误响应，而不影响同批次其他请求的正常结果。理想情况下，模型推理库本身能提供**逐样本错误报告**；如果没有，则需要在业务逻辑层对输入进行校验，避免非法输入进入批处理。

* **超时和取消**:当某些请求被用户取消或客户端断开时，应支持将其从批队列中移除，避免占用计算资源。实现上可以在请求Future对象取消时，从队列中剔除对应项。如果取消发生在已送入GPU执行后，则只能尽快丢弃结果。在超时场景下(请求级超时)，也类似处理。确保**取消的请求不影响批处理**流程即可。

* **资源异常**:密切关注GPU OOM、驱动错误等严重异常。如果出现GPU OOM，可以尝试自动**减小批大小**重试一次；如果连续OOM，则暂停接受新请求并告警运维。另外准备应急预案，例如GPU故障时将流量切换到其它节点(需要有冗余实例)。

通过完善的异常处理机制，动态批处理系统才能在各种不确定情况下**优雅地降级**而不崩溃，从而提供生产级的可靠服务。

### 监控指标

为了评估和优化批处理效果，需要定义和跟踪一系列**关键性能指标**
* **平均批大小**:每次实际执行的批请求数的平均值。这个指标反映批处理的“打包效率”。接近设定的最大批大小说明大部分时间GPU都满载工作，利用率高；过小则说明要么流量不足，要么等待超时太短。可以根据该指标调整批大小和超时参数。

* **请求延迟**:包括**端到端延迟**(从请求发出到结果返回)以及其中的**排队延迟**和**推理时间**等。重点关注P95/P99等尾延迟，观察批处理是否引入长尾(例如某些请求等了多个批周期才处理)。如果发现延迟分布长尾很长，可能需要减小等待超时或采用优先级队列保证及时性。

* **吞吐量**:通常以每秒处理的请求数或token数来衡量。LLM生成任务可以用每秒输出token数来衡量实际效能。吞吐量直接体现批处理带来的加速比，可结合延迟指标一起分析取舍。例如Databricks的测试表明，将批大小增至64在A100上可将吞吐提升14倍，但单请求延迟也增加4倍。服务提供者应根据自身应用找到合适的吞吐/延迟平衡点。

* **资源利用率**:包括GPU算力利用率、显存占用率、CPU利用率等。理想情况下GPU利用率应维持在较高水平(比如80%以上)且较平稳。如果利用率经常偏低，说明批处理仍有改进空间或硬件算力富余；如果利用率100%且任务积压，则说明系统可能超载需扩容。显存占用接近上限时要警惕OOM风险，CPU成为瓶颈时考虑提升I/O或预处理效率等。

* **错误率**:每分钟/每小时失败的请求数占比。这个指标应尽量趋近于0，一旦升高表明可能发生了批处理Bug或下游模型错误，需要及时定位。分类统计错误原因(超时、OOM、模型异常等)有助于针对性优化。

通过仪表盘持续监控这些指标，运维和开发人员可以及时发现问题、验证调整效果。监控数据也为**性能调优**提供了量化依据。

### 批处理性能调优

批处理策略的优化通常围绕**批大小**这个关键参数展开，因为它直接影响吞吐与延迟。不同批大小对系统性能的典型影响如下表所示
| 批大小 | 吞吐量提升 (相对单请求) | 延迟增加 (相对单请求) | 适用场景 |
| -------- | ------------- | ------------ | ------------------ |
| 小 (1-4) | `1-2倍` | &lt;10ms 微增 | **低延迟优先**场景，交互式请求 |
| 中 (8-16) | `2-4倍` | 20-50ms 增加 | **平衡**吞吐与延迟，一般网络服务 |
| 大 (32及以上) | `4-8倍` | &gt;100ms 明显 | **高吞吐优先**场景，离线批处理 |

*注:以上为经验值，具体增益视模型和硬件而定。例如在一块NVIDIA A100 GPU上，将批大小从1增加到64，吞吐量曾测得提高约14倍，而延迟增加约4倍。因此实际应用需根据对延迟的容忍度选择合适的批次大小。*

从表中可以看出，小批次几乎不影响单请求响应时间，但合并效果有限；超大批次则极大提升了整体 throughput，但会令每个请求等待更长时间。**共享在线服务**一般取中等批次做权衡，而如果用户自托管模型，可根据自身需求决定偏向低延迟还是高吞吐。

调优批大小的实用步骤是:先在测试环境以不同批大小跑基准测试，绘制**吞吐-延迟曲线**。通常曲线在某点后趋于平坦甚至变差(达到计算上限后，再增大batch只会线性拉长延迟而几乎不提高吞吐)。选取拐点前的批大小作为上线配置起点。之后在真实流量中监控并微调，以适应实际的序列长度分布和流量模式。对于LLM这类迭代生成任务，还应考虑**单token处理时间**随batch变化的影响，并留意极端长序列对有效batch的削减(因为序列太长会限制batch能容纳的请求数)。

此外，可以尝试**分组批处理**等先进方法。例如**多队列分桶(Multi-bin Batching)**思想是将请求按预估输出长度分类，长输出的放一批、短的放一批，从而避免长尾请求拖慢整个批完成时间。这种方法本质上也是提升batch内请求的同质性，减少变异带来的等待开销。实际中这可以由调度器基于历史交互长度或用户设定的max tokens字段来预分配队列。

总之，性能调优是一个反复迭代的过程: **基准测试 -> 监控分析 -> 参数调整**，再循环验证。随着模型和硬件的演进(例如新GPU的内存带宽提升，可能支持更大batch)，需要不断更新优化策略以充分利用新性能。持续的profiling和优化能确保批处理系统始终运行在**高效稳健**的状态。

### 常见问题与对策

即使有完善的设计和调优，LLM批处理部署中仍会遇到一些常见挑战。下面总结几点以及相应的应对策略
* **延迟过高**:当监控发现请求延迟(特别是p95/p99)超出要求时，首先考虑**减小最大批大小**或**缩短批处理等待时间**来减少排队延迟。此外，可以**增加并行处理实例**(水平扩容)分担负载。确保启用了优先级队列后，关键请求不被长时间阻塞。如果延迟问题集中在生成长序列的请求上，可以引入连续批处理或多队列策略，让短请求不必等待长请求完成。

* **内存溢出(OOM)**:发生GPU内存不足错误时，可采取**动态调整批大小**降低单批占用，或者在启动时估计更保守的批次上限。定期**清理缓存显存**和未用对象，减少碎片堆积(例如每处理一定数量请求后调用一次显存清理)。监控显存利用率曲线，如果接近100%应及时扩充硬件或降低负载。对于超长序列请求，可以限制最大生成长度或使用分段策略防止KV缓存无限增长。

* **请求堆积**:当请求队列长度持续增加，响应变慢，说明系统**处理能力不足**。此时需要**启用降级策略**，例如在队列超过阈值时对新请求直接走单独处理或简单应答，以**快速消费积压**。同时考虑**增加计算实例或GPU**来提高总体吞吐能力。检查调度算法是否高效，有无因为锁或单一队列导致瓶颈——可改进为多队列并发取、减少临界区等待。此外，如果堆积有周期性峰值，可以预先通过自动扩容在高峰到来前增加实例，峰过后再收缩。

通过以上措施，绝大多数批处理中的难题都能迎刃而解。LLM批处理技术需要在**系统工程实践**中不断演进，根据最新研究成果(如连续批处理、内存感知调度等)优化策略。总结来说，只有全面考虑从算法原理、代码实现到运行时调度和监控的各个方面，并结合具体应用场景调整，才能将批处理的威力发挥到极致，在保障服务质量的同时实现令人满意的性能和效率提升。
