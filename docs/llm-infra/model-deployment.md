---
id: model-deployment
sidebar_position: 3
---

# 模型部署

## 容器化部署（Docker/Kubernetes）

**定义：**使用容器技术将模型及其依赖环境封装打包，实现一次构建、多处运行的可移植部署方式。结合容器编排（如Kubernetes），可以在集群上自动部署和管理模型服务实例。

**用途：**容器化可以解决"环境配置不一致"的问题，将模型所需的库、文件乃至GPU驱动都封装在镜像内，确保无论在开发机还是线上集群运行，都有相同的运行环境。这大大提高了部署可靠性和效率。Docker容器启动较虚拟机更轻量，便于弹性伸缩。Kubernetes作为容器编排平台，可以管理成百上千容器实例，对模型服务进行自动重启、伸缩、服务发现和负载均衡。对于需要并发多实例的模型部署，K8s几乎是标配方案。

**典型工具/实现：**使用Dockerfile制作模型服务镜像，将模型文件、依赖包安装等在镜像构建阶段完成。部署时，在Kubernetes集群创建Deployment或Service来运行容器。通过K8s Device Plugin挂载GPU，保证容器内部能访问宿主机GPU资源。也可使用Kubernetes Operators（如Kubeflow、Volcano等）简化部署配置。在云厂商上，还可直接利用托管容器服务（如AWS SageMaker、GCP Vertex AI）进行模型托管，这些服务底层也是通过容器运行。

**注意事项：**容器化模型需关注镜像体积——包含大模型权重的镜像往往十几GB，部署时拉取开销大，可以考虑模型与代码分开，通过启动时再下载模型权重，或使用分层镜像将基础环境和模型文件拆开。GPU容器需确保驱动和CUDA库兼容，可使用NVIDIA提供的CUDA基础镜像来构建。Kubernetes部署时编排参数也很关键，如Requests/Limits资源请求，设置合理值确保Pod调度到有足够GPU内存的节点。最好启用**就绪探针**（Readiness Probe），在模型加载完毕后才对外宣称可用，防止容器启动期间就接受请求导致失败。安全方面，容器隔离有助于不同模型环境互不干扰，但仍需遵循原则最小化权限（避免容器以root运行等）。

## 持续集成/持续部署（CI/CD）流程

**定义：**将模型的构建、测试、部署步骤自动化的流程，通常包括持续集成（CI）阶段自动完成代码和模型的构建与验证，以及持续部署（CD）阶段自动将通过验证的新版模型部署到生产环境。

**用途：**CI/CD可以使模型更新迭代更敏捷可控。当开发者提交新的模型代码或参数时，CI流水线会触发构建（比如打包新的容器镜像）、运行单元测试和集成测试（例如验证新模型的输出是否符合预期、性能是否达标），然后CD流程会逐步部署该版本——通常先到测试/预发布环境，再推到生产。对教学或多团队协作而言，CI/CD确保每次部署过程一致、可重复，减少人为操作失误。对于LLM服务，由于模型体积大，可以在CI阶段就验证模型可以正常load、小样本推理通过，再进入部署，提高上线质量。

**典型工具/实现：**Jenkins、GitLab CI、GitHub Actions等常用于CI部分，例如配置在代码库有变更时执行构建脚本：安装依赖、加载模型权重、运行一系列pytest或校验脚本。CD部分常用Spinnaker、Argo CD、Jenkins X等，或云厂商的CD服务。以Kubernetes环境为例，CD流程可能通过`kubectl apply`或Helm升级来实现模型服务的滚动更新。当模型artifact较大（如上GB级别）时，也要考虑CI/CD环境的存储和网络，通过缓存和增量构建减少每次处理时间。

**注意事项：**设计CI/CD需关注**测试质量**。对于模型，不仅代码要测，新模型本身指标也要测——例如BLEU、准确率、推理延迟等，可以在CI阶段跑一小批验证数据比对metrics，或集成模型卡验证。持续部署时，应有发布审批机制或分阶段发布（结合A/B测试、灰度发布，见下节）避免有缺陷模型直接全量上线。同时，CI/CD环境要具备和生产相似的运行依赖，特别是GPU/TPU资源：建议在CI中配置带GPU的runner，真正执行一次模型加载和推理，这比单纯模拟更保险。最后，大模型构建发布流程时间长，要优化流水线如并行步骤、按需执行（无改动则跳过）等，以免一次CI跑太久影响团队效率。

## 自动伸缩与资源管理

**定义：**根据服务负载（请求量、响应时间等）自动调整模型服务实例数量或资源分配的机制，确保既能在高峰时提供足够算力保持性能，又能在低谷时释放多余资源节省成本。资源管理则指合理规划分配GPU、CPU等给不同服务，使集群资源利用率最大化。

**用途：**对于流量不恒定的模型API，自动伸缩（Auto Scaling）能按需增减实例数。例如配置Kubernetes HPA（Horizontal Pod Autoscaler），以CPU/GPU利用率或队列长度为指标，当指标高于阈值则自动创建新Pod扩容，当低于阈值则缩容。这样白天高并发时模型服务能扩展到多副本并行处理请求，半夜无人访问时则降到少量甚至零实例，减少云计算费用。资源管理还包括在同一集群上多模型共存调度，如给重要模型预留GPU、将低优先级任务调度到空闲GPU上，或使用分片GPU（MPS、MIG）同时跑多个轻量模型。

**典型工具/实现：**
- **Kubernetes HPA/VPA：** HPA根据Pod的metrics（可自定义，如QPS、延迟）调整副本数。Vertical Pod Autoscaler (VPA)则可自动调整容器的资源Requests值，对长期负载模式有帮助。
- **云厂商Auto Scaling Group：** 在VM级别，如AWS EC2 Auto Scaling可随负载增减GPU服务器节点，结合K8s Cluster Autoscaler实现跨节点伸缩。
- **Ray Autoscaler：** Ray集群可以根据任务队列自动增加节点（例如spin up新的云实例）或remove空闲节点。对于Ray Serve部署的模型，这能做到弹性使用算力。
- **资源配额与池化：** 利用K8s Namespace配额限制不同团队的GPU用量，或使用资源调度插件（如Apache YARN、Volcano）做更细粒度的AI作业调度。也可以将GPU按时间段或优先级分配（例如夜间允许离线批作业占满GPU，白天留给在线服务）。

**注意事项：**自动伸缩需要平衡**伸缩速度**与**稳定性**。收集指标通常有延迟，伸缩动作本身（启动容器和加载模型）也需要时间。因此参数设置（如HPA的cooldown）要避免频繁抖动。另外，大模型实例启动慢（冷启动问题），可采用预测式扩容（predictive scaling）或保持minReplicas来快速响应突发。对有状态的对话模型，还要考虑会话固定到某实例的问题，伸缩时尽量不终止活跃会话。资源管理方面，若一个节点运行多个模型容器，要确保显存和CUDA流隔离，防止互相抢占导致OOM或延迟飙升。可以利用nvidia-cgroups或MIG将GPU资源划分硬隔离给容器。总体来说，实现良好的弹性需要对模型负载模式充分了解，并做充足测试以找到最佳策略。

## A/B测试与蓝绿部署

**定义：**A/B测试是在部署新模型或新版本时，将一部分用户流量引导给新模型（版本B），另一部分仍由旧模型（版本A）服务，从而比较两者在真实环境下的效果差异。蓝绿部署是一种发布策略：同时运行两套环境（蓝=当前生产版本，绿=新版本），绿环境经过充分测试后通过切换路由将流量从蓝切到绿，实现无缝升级和快速回滚。

**用途：**A/B测试主要用于评估新模型性能，如比较新模型响应质量、用户转化率等指标是否优于旧模型。在LLM场景，新模型可能提升回答准确率，但也可能引入延迟或错误，A/B测试能定量分析效果，给出上线与否的决策依据。蓝绿部署侧重于**稳定发布**：通过在绿环境预先部署新模型服务，让其在与蓝环境相同的生产配置下运行（但不接正式用户），进行烟囱测试或试跑，然后流量一次性全部切换。这确保发布过程几乎无停机，而且如新版本出问题可以快速切回蓝环境。

**典型流程：**在Kubernetes上，可以用Deployment和Service实现蓝绿：先部署新Deployment（绿）并将其Service设置不对外，然后当需要切换时，更新服务的selector指向绿Deployment的标签。一旦切换确认稳定，可以关掉蓝Deployment。同样，也可以使用Istio等服务网格，把一小部分流量分到绿版本做金丝雀发布/灰度（类似A/B测试）。A/B测试实现时，通常由调用方或网关随机将用户会话分配版本标签A或B，通过HTTP请求头或参数决定路由。数据收集方面，要在日志或监控中记录版本标签，以比较指标。

**注意事项：**A/B测试要求同时运行两套模型，资源开销增大，要确保有足够算力支撑。测试期间要尽可能保证除模型本身外其他因素一致（如缓存、数据库访问等），以免干扰结果。样本量和测试时长也需要足够，否则差异可能不具有统计显著性。蓝绿部署需要警惕**数据一致性**问题：如果新版本对数据库有变更或不兼容，需要在切换时同步数据或设计迁移机制。一旦切换到绿版本，在确认稳定前不要立即删除蓝版本，以便随时回滚。无论A/B还是蓝绿，都应有监控告警设置：如发现新模型导致错误率或延迟上升，应及时减流量或回滚。通过这些策略，可以降低新模型上线的风险，将潜在问题影响范围控制在可控范围。

## 在线/离线混合架构

**定义：**在线服务系统与离线批处理系统相结合的架构模式。在AI应用中，通常将实时要求高的部分放在在线系统（Serving）中，而将计算量大但时效要求低的部分放在离线流程中，由此充分利用资源并优化延迟。例如推荐系统经典架构中，离线阶段定期跑模型训练和特征处理，在线阶段使用离线生成的模型或索引进行实时推荐查询。

**用途：**对LLM应用而言，混合架构可体现在多个方面：
- **模型训练/更新：** 大模型的训练fine-tune通常离线进行，然后将生成的模型快照上线供实时查询使用。训练所需的大量算力无需在线持续投入，只需在离线环境定期完成即可。
- **批量预计算：** 一些推理结果可以提前算好。例如针对常见问题或热门输入，可以离线算出答案缓存起来，在线查询时若命中缓存即可直接返回，减少实际模型推理调用。
- **索引与检索：** 在检索增强生成（RAG）场景中，会先离线将知识库文本编码为向量索引，然后在线查询时仅进行向量检索+小模型匹配，节省每次都编码大量文本的成本。
- **多级架构：** 前端在线系统接收请求后，可能调用后台多个模块，其中一些模块是流式在线处理，另一些则将任务打包异步交给离线作业。例如用户提交一段文章请求摘要，系统可以先返回一个粗略快速摘要（在线小模型），再异步调用一个大模型离线生成精细摘要，完成后再更新给用户。

**典型实现：**常见是Lambda架构（批处理+实时处理）在AI场景的应用。比如对于监控大量社交媒体消息的总结模型，可以每小时批量抓取和初步分类（离线），将结果存储，然后用户查询时在短文本上做实时生成。技术上，离线部分可利用大数据平台（Hadoop/Spark或更现代的Ray Batch），在线部分用微服务架构。两者通过中间存储衔接（数据库、消息队列等）。一个例子是搜索引擎问答：离线阶段生成问答对或embedding索引，在线通过这些索引更快锁定相关内容，再用LLM生成答案。

**注意事项：**在线/离线划分的核心在于**时效和平衡**。需要明确哪些计算可延后、预先进行而不会显著影响结果时效性。对于预计算内容，要处理数据陈旧问题（如知识更新）。可以采用定期重跑离线任务或者引入在线校正机制。在架构设计上，要保证离线产出的数据格式和版本与在线模型兼容，避免出现线上模型读不懂离线结果的情况。最后，监控需覆盖离线流程，万一离线任务失败或延迟，也要及时通知运维，否则线上系统可能拿不到更新的数据从而性能退化。通过协同在线快速响应能力和离线深度计算能力，混合架构能够兼顾性能与效果，在教学和真实生产中都十分常用。 