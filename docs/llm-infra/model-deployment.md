---
id: model-deployment
sidebar_position: 3
---

# 模型部署

## 容器化部署(Docker/Kubernetes)

**定义:** 使用容器技术将模型及其依赖环境封装打包，实现一次构建、多处运行的可移植部署方式。结合容器编排(如Kubernetes)自动在集群上部署和管理模型服务实例。

**用途:** 容器化能够解决"环境配置不一致"的问题，将模型的库文件、配置甚至GPU驱动都打包进镜像中，确保无论在开发机还是生产集群中运行都有相同的运行时环境。这极大提升了部署的可靠性和效率。Docker容器比虚拟机更轻量，弹性扩缩更方便。Kubernetes作为容器编排平台，能够管理成百上千个容器实例，自动重启、扩缩、服务发现和负载均衡模型服务。对于需要多实例的模型，K8s几乎是默认选择。

**典型工具/实现:** 使用Dockerfile创建模型服务镜像，在镜像构建阶段完成模型文件和依赖安装。通过在Kubernetes集群中创建Deployment或Service部署。使用K8s Device Plugin挂载GPU资源，确保容器能访问宿主机的GPU资源。也可以使用Kubernetes Operator(如Kubeflow、Volcano)简化部署配置。在云平台上，可以直接使用托管容器服务(如AWS SageMaker、GCP Vertex AI)进行模型托管。这些服务底层也是运行在容器上。

**注意事项:** 容器化模型需要注意镜像大小——包含大模型权重的镜像往往几十GB，部署时拉取代价不小。可以考虑模型与代码分离，启动时下载模型权重，或者使用多阶段镜像分离基础环境和模型文件。GPU容器需要确保驱动与CUDA库兼容，可以使用NVIDIA的CUDA基础镜像构建。Kubernetes部署配置也很关键，如资源请求Requests/Limits，设置合理数值确保Pod调度到有足够GPU显存的节点。最好启用**就绪探针**(Readiness Probe)，在模型加载完成后才声明可用，防止容器启动过程中接受请求导致失败。安全方面，容器隔离有助于不同模型环境互不干扰，但仍需遵循最小权限原则(避免以root运行容器等)。

## 持续集成/持续部署(CI/CD)流程

**定义:** 将模型构建、测试、部署的步骤自动化的流程，通常包括CI阶段自动完成代码和模型的构建验证，CD阶段自动将验证通过的新版本部署到生产环境。

**用途:** CI/CD让模型更新迭代更加敏捷可控。当开发者提交新模型代码或参数时，CI流水线触发构建(如打包新容器镜像)，运行单元测试和集成测试(如验证新模型输出是否符合预期，性能要求等)，然后CD流程逐步部署版本——通常先到测试/预发环境，再到生产。对于教学或多团队协作，CI/CD确保每次部署过程一致且可重复，减少人工操作错误。对于LLM服务，由于模型体积大，可以在CI阶段验证模型能正常加载并通过小样本推理后再部署，提高模型上线质量。

**典型工具/实现:** 常用Jenkins、GitLab CI、GitHub Actions等做CI，如配置构建脚本在代码库变更时安装依赖、加载模型权重并运行一系列pytest或验证脚本。CD部分常用Spinnaker、Argo CD、Jenkins X或云平台的CD服务。在Kubernetes环境中，CD流程可通过kubectl apply或Helm upgrade实现模型服务的滚动更新。当模型artifact较大(如GB级别以上)时，还需考虑CI/CD环境的存储和网络，通过缓存和增量构建减少处理时间。

**注意事项:** 设计CI/CD需要注意**测试质量**。对于模型，不仅要测试代码，还要测试新模型指标——如BLEU、准确率、推理延迟等。可以在CI阶段跑小批量验证数据对比指标，或者集成模型卡验证。持续部署时应有发布审批机制或分阶段发布(结合A/B测试、灰度发布，见下节)，避免有缺陷的模型全量部署。同时CI/CD环境需要与生产有相似的运行时依赖，特别是GPU/TPU资源:建议在CI中配置带GPU的runner真正执行模型加载和推理，比单纯模拟更安全。最后，由于模型构建发布流程较长，需要优化流水线，如并行步骤、按需执行(无变更则跳过)等，避免过长的CI运行影响团队效率。

## 自动扩缩容与资源管理

**定义:** 根据服务负载(如请求量、响应时间等)自动调整模型服务实例数量或资源分配的机制，确保高峰时提供足够算力维持性能，低谷时释放多余资源节约成本。资源管理指合理分配GPU、CPU等给不同服务，最大化集群资源利用率。

**用途:** 对于流量有波动的模型，自动扩缩容(Auto Scaling)可以按需调整实例数。例如配置Kubernetes HPA(Horizontal Pod Autoscaler)在CPU/GPU利用率或队列长度超过阈值时自动创建新Pod扩容，低于阈值时缩容。这样模型服务在白天高并发时可以扩展到多个副本实例并行处理，夜间无访问时缩减到少量甚至零实例。资源管理还包括同一集群上多模型共存调度，如为重要模型预留GPU，将低优先级任务调度到空闲GPU，或使用GPU分割(MPS、MIG)让多个轻量模型同时运行。

**典型工具/实现:** 
- **Kubernetes HPA/VPA:** HPA根据Pod指标(可自定义，如QPS、延迟)调整副本数。Vertical Pod Autoscaler(VPA)可以自动调整容器的资源Requests值，对长期负载模式有帮助。
- **云平台Auto Scaling Group:** VM级别的，如AWS EC2 Auto Scaling，可以随负载调整GPU服务器节点，结合K8s Cluster Autoscaler实现跨节点扩缩。
- **Ray Autoscaler:** Ray集群可以根据任务队列自动增加节点(如拉起新云实例)或移除空闲节点。对于用Ray Serve部署的模型，这样可以实现算力的弹性使用。
- **资源配额和池化:** 使用K8s Namespace quota限制不同团队的GPU使用，或者使用资源调度插件(如Apache YARN、Volcano)进行更细粒度的AI作业调度。也可以按时间段或优先级分配GPU(如允许离线批量作业夜间占用全部GPU，白天留给在线服务)。

**注意事项:** 自动扩缩需要平衡**扩缩速度**与**稳定性**。指标收集通常有延迟，扩缩动作(容器启动、模型加载)也需要时间。因此参数设置(如HPA的冷却期)需要避免频繁震荡。此外，大模型实例启动较慢(冷启动问题)，可以采用预测性扩缩或保持minReplicas以快速响应突发变化。对于有状态的对话模型，还需考虑扩缩时会话固定到某个实例的问题，尽量避免缩容时终止活跃会话。资源管理方面，如果一个节点运行多个模型容器，需要确保GPU和CUDA stream隔离分离，防止互相抢占导致OOM或延迟飙升。可以使用nvidia-cgroups或MIG对容器硬隔离GPU资源。总体而言，实现良好弹性需要充分了解模型负载模式并做充分测试，找到最佳策略。

## A/B测试与蓝绿部署

**定义:** A/B测试是在部署新模型或版本时，将部分用户流量导向新模型(版本B)，其余部分仍由旧模型(版本A)服务，对比真实环境下的效果差异。蓝绿部署是一种发布策略:同时运行两套环境(蓝=当前生产版本，绿=新版本)，绿环境充分测试后将流量从蓝切换到绿，实现无缝升级和快速回滚。

**用途:** A/B测试主要用于评估新模型的表现，如对比新模型的回答质量、用户转化率是否优于旧模型。在LLM场景中，虽然新模型可能提升回答准确性，但也可能引入延迟或错误。A/B测试可以定量分析效果，为是否上线提供决策依据。蓝绿部署重点在于**稳定发布**:通过在绿环境部署新模型服务但不接收正式用户前(蓝环境仍在服务)，进行烟囱测试或试运行，然后一次性切换流量。这保证发布过程几乎无停机时间，且新版本有问题时可以快速切回蓝环境。

**典型流程:** 在Kubernetes中，可以使用Deployment和Service实现蓝绿:先部署新的Deployment(绿)，设置其Service不暴露，需要切换时更新service selector指向绿Deployment的label。确认切换稳定后可以关闭蓝Deployment。同样，可以使用Istio等服务网格将小部分流量分流到绿版本进行金丝雀发布/灰度(类似A/B测试)。实现A/B测试时，通常由调用方或网关根据HTTP请求头或参数随机分配用户会话的版本标签决定路由。数据收集方面，需要在日志或监控中记录版本标签以供对比。

**注意事项:** A/B测试需要同时运行两个模型，增加资源消耗，需要确保有足够算力支撑。测试时需确保除模型本身外其他因素一致(如缓存、数据库访问等)，否则结果可能受影响。样本量和测试时长也需要充足，否则差异可能无统计意义。蓝绿部署需要警惕**数据一致性**问题:如果新版本对数据库有变更或不兼容，切换时需要同步数据或设计迁移机制。切换到绿版本后，确认稳定前不要立即删除蓝版本，以便随时回滚。无论A/B还是蓝绿，都应有监控和告警设置:发现新模型导致错误率或延迟上升应立即减少流量或回滚。通过这些策略，可以降低新模型上线风险，将潜在问题控制在可控范围内。

## 在线/离线混合架构

**定义:** 结合在线服务系统和离线批处理系统的架构模式。在AI应用中，通常把实时性要求高的部分放在在线系统(Serving)，把计算量大但时效性要求低的部分置于离线过程，从而充分利用资源并优化延迟。例如推荐系统的经典架构中，离线阶段定期运行模型训练和特征处理，在线阶段使用离线生成的模型或索引进行实时推荐查询。

**用途:** 对于LLM应用，混合架构可体现在多个方面:
- **模型训练/更新:** 大模型训练微调通常离线进行，然后生成模型快照在线提供实时查询。训练所需的大算力不需要持续投入在线，只需在离线环境完成即可。
- **批量预计算:** 一些推理结果可以提前计算。例如对于常见问题或热门输入，可以离线预先计算答案并缓存，在线查询时如果命中缓存直接返回，减少实际模型推理调用。
- **索引与检索:** 在检索增强生成(RAG)场景中，先离线将知识库文本编码为向量索引，然后在线只进行向量检索+小模型匹配，省去每次编码大文本的成本。
- **多层架构:** 前端在线系统接收请求后，可能后台调用多个模块，有些是流式在线处理，有些将任务打包异步给离线作业。例如用户提交文章摘要请求时，系统可先返回粗略快速摘要(在线小模型)，然后异步调用大模型离线生成精细摘要，完成后更新给用户。

**典型实现:** 常见的是Lambda架构(批处理+实时处理)在AI场景的应用。例如对于汇总大量社交媒体消息的摘要模型，可以每小时批量抓取并初步分类(离线)，存储结果，然后用户查询时对短文本使用实时生成。技术上，离线部分可以使用大数据平台(Hadoop/Spark或更现代的Ray Batch)，在线部分使用微服务架构。两者通过中间存储连接(数据库、消息队列等)。一个例子是搜索引擎问答:离线阶段生成问答对或embedding索引，在线通过这些索引更快锁定相关内容，然后用LLM生成答案。

**注意事项:** 在线/离线拆分的核心是**时效性与平衡**。需要清楚理解哪些计算可以延迟和预先执行而不显著影响结果时效性。对于预计算内容，需要处理数据陈旧性问题(如知识更新)。可以通过定期重跑离线任务或引入在线校正机制。架构设计中，需要确保离线输出与在线模型的数据格式和版本一致性，避免在线模型看不懂离线结果的情况。最后，监控需要覆盖离线过程，如果离线任务失败或延迟，也需要通知运维，否则在线系统可能拿不到更新数据而性能下降。通过协同在线快速响应能力和离线深度计算能力，混合架构可以兼顾性能与效果，在教学和真实生产中都十分常用。 