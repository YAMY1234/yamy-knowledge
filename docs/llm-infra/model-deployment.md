---
id: model-deployment
sidebar_position: 3
---

# 模型部署

## 容器化部署(Docker/Kubernetes)

**定义:** 使用容器技术将模型及其依赖环境封装打包，实现一次构建、多处运行的可移植部署方式. Combine container orchestration (such as Kubernetes) to automatically deploy and manage model service instances on a cluster.

**用途:** Containerization can solve the problem of "environment configuration inconsistency" by packaging the model's libraries, files, and even GPU drivers into the image, ensuring the same runtime environment whether the model is running on a development machine or a production cluster. This greatly improves deployment reliability and efficiency. Docker containers are lighter than virtual machines, making them easier to scale elastically. Kubernetes, as a container orchestration platform, can manage hundreds or thousands of container instances, automatically restarting, scaling, discovering, and load balancing model services. For models that require multiple instances, K8s is almost a default solution.

**Typical tools/implementations:** Use Dockerfile to create a model service image, completing the installation of model files and dependencies during the image build phase. Deploy by creating a Deployment or Service in the Kubernetes cluster. Mount GPU resources using K8s Device Plugin to ensure containers can access GPU resources on the host machine. You can also use Kubernetes Operators (such as Kubeflow, Volcano) to simplify deployment configuration. On cloud platforms, you can directly use managed container services (such as AWS SageMaker, GCP Vertex AI) for model hosting. These services also run on containers.

**注意事项:** Containerized models need to pay attention to image size—images containing large model weights are often tens of GB, and pulling them during deployment can be costly. You can consider separating the model and code, downloading the model weights when starting, or using multi-stage images to separate the base environment and model files. GPU containers need to ensure compatibility between drivers and CUDA libraries. You can use NVIDIA's CUDA base image to build it. Kubernetes deployment configuration is also critical, such as Requests/Limits resource requests, setting reasonable values to ensure Pods are scheduled to nodes with sufficient GPU memory. It's best to enable **readiness probe** (Readiness Probe), claiming the model is ready to use only after it has finished loading, to prevent failure due to accepting requests during container startup. Security-wise, container isolation helps different model environments not interfere with each other, but you still need to follow the principle of minimizing permissions (avoiding running containers as root, etc.).

## Continuous Integration/Continuous Deployment (CI/CD) Process

**Definition:** The process of automating the steps of building, testing, and deploying a model, usually including the CI stage automatically completing the build and validation of code and model, and the CD stage automatically deploying the newly validated version to production.

**Purpose:** CI/CD allows for more agile and controllable model updates and iterations. When developers submit new model code or parameters, the CI pipeline triggers the build (such as packaging a new container image), runs unit tests and integration tests (such as verifying whether the output of the new model meets expectations and performance requirements), and then the CD process gradually deploys the version—usually first to a test/pre-release environment, then to production. For teaching or multi-team collaboration, CI/CD ensures that each deployment process is consistent and repeatable, reducing human operation errors. For LLM services, since the model volume is large, you can verify the model can load normally and pass small sample inference in the CI stage before deployment, improving the quality of the model's online launch.

**Typical tools/implementations:** Jenkins, GitLab CI, GitHub Actions, etc. are commonly used for CI, such as configuring build scripts to install dependencies, load model weights, and run a series of pytest or verification scripts when there are codebase changes. CD parts commonly use Spinnaker, Argo CD, Jenkins X, or CD services from cloud platforms. In a Kubernetes environment, CD processes can be implemented through `kubectl apply` or Helm upgrade to implement rolling updates for model services. When the model artifact is large (such as above GB level), you also need to consider storage and network for CI/CD environments, reducing processing time through caching and incremental build.

**注意事项:** Design CI/CD needs to pay attention to **test quality**. For models, not only code needs to be tested, but also new model metrics need to be tested—for example, BLEU, accuracy, inference latency, etc. You can run a small batch of validation data to compare metrics in the CI stage, or integrate model card verification. During continuous deployment, there should be a release approval mechanism or phased release (combined with A/B testing, gray release, see next section) to avoid defective models being deployed in full. Meanwhile, CI/CD environments need to have similar runtime dependencies as production, especially GPU/TPU resources: It's recommended to configure a runner with GPU in CI to truly execute model loading and inference, which is safer than simple simulation. Finally, since the model build and release process is long, you need to optimize the pipeline, such as parallel steps and on-demand execution (skip if no changes), to avoid a long CI run affecting team efficiency.

## Auto Scaling and Resource Management

**Definition:** The mechanism of automatically adjusting the number of model service instances or resource allocation based on service load (such as request volume, response time, etc.), ensuring that enough computing power is provided during peak hours to maintain performance, and releasing extra resources to save costs during low hours. Resource management refers to reasonably allocating GPU, CPU, etc. to different services to maximize cluster resource utilization.

**Purpose:** For models with fluctuating traffic, auto scaling (Auto Scaling) can adjust the number of instances as needed. For example, configure Kubernetes HPA (Horizontal Pod Autoscaler) to automatically create new Pods to expand when the CPU/GPU utilization rate or queue length exceeds the threshold, and scale in when it falls below the threshold. This allows the model service to expand to multiple replica instances for parallel processing during the day when there is high concurrency, and reduce to a small number or even zero instances during the night when there is no access. Resource management also includes multi-model coexistence scheduling on the same cluster, such as reserving GPU for important models, scheduling low priority tasks to idle GPU, or using split GPU (MPS, MIG) to run multiple lightweight models simultaneously.

**Typical tools/implementations:**
- **Kubernetes HPA/VPA:** HPA adjusts the number of replicas based on Pod metrics (customizable, such as QPS, latency). Vertical Pod Autoscaler (VPA) can automatically adjust the resource Requests value of containers, which is helpful for long-term load mode.
- **Cloud platform Auto Scaling Group:** At the VM level, such as AWS EC2 Auto Scaling, which can adjust GPU server nodes with load, combined with K8s Cluster Autoscaler to achieve cross-node scaling.
- **Ray Autoscaler:** Ray cluster can automatically increase nodes (such as spinning up new cloud instances) or remove idle nodes based on task queue. For models deployed with Ray Serve, this can achieve elastic use of computing power.
- **Resource Quota and Pooling:** Use K8s Namespace quota to limit GPU usage for different teams, or use resource scheduling plugins (such as Apache YARN, Volcano) for more granular AI job scheduling. You can also allocate GPU by time period or priority (such as allowing offline batch jobs to occupy full GPU at night and leaving it for online services during the day).

**注意事项:** Auto scaling needs to balance **scaling speed** and **stability**. Collection of indicators usually has latency, and scaling actions (container startup and model loading) also need time. Therefore, parameter settings (such as HPA's cooldown) need to avoid frequent fluctuations. In addition, large model instances start slowly (cold start problem), you can use predictive scaling or keep minReplicas to quickly respond to sudden changes. For stateful dialogue models, you also need to consider the problem of session fixed to a certain instance when scaling, try to avoid terminating active sessions when scaling. Resource management-wise, if a node runs multiple model containers, you need to ensure separate GPU and CUDA stream isolation, preventing mutual抢占导致OOM或延迟飙升。 You can use nvidia-cgroups or MIG to hard isolate GPU resources for containers. Overall, to achieve good elasticity, you need to fully understand the model load mode and do sufficient testing to find the best strategy.

## A/B Testing and Blue-Green Deployment

**Definition:** A/B testing is when a new model or version is deployed, part of the user traffic is directed to the new model (version B), and the other part is still served by the old model (version A), comparing the differences in effect in the real environment. Blue-green deployment is a release strategy: simultaneously running two environments (blue = current production version, green = new version), green environment passes traffic from blue to green after fully testing, achieving seamless upgrade and quick rollback.

**Purpose:** A/B testing is mainly used to evaluate the performance of a new model, such as comparing whether the new model's response quality and user conversion rate are better than the old model. In the LLM scenario, although the new model may improve the accuracy of answers, it may also introduce latency or errors. A/B testing can quantitatively analyze the effect, providing decision-making basis for whether to launch. Blue-green deployment focuses on **stable release**: by deploying the new model service in the green environment before the blue environment (but not receiving official users), performing chimney test or trial run, and then switching traffic all at once. This ensures that the release process is almost without downtime, and if the new version has problems, you can quickly switch back to the blue environment.

**Typical process:** In Kubernetes, you can use Deployment and Service to implement blue-green: first deploy the new Deployment (green) and set its Service to not expose it, then when you need to switch, update the service selector to point to the label of the green Deployment. Once the switch is confirmed stable, you can turn off the blue Deployment. Similarly, you can use Istio and other service meshes to split a small part of traffic to the green version for canary release/gray (similar to A/B testing). When implementing A/B testing, the version label of the user session is usually assigned randomly by the caller or gateway based on HTTP request headers or parameters to decide routing. Data collection-wise, you need to record version labels in logs or monitoring for comparison.

**注意事项:** A/B testing requires running two models simultaneously, increasing resource consumption, and you need to ensure enough computing power to support it. During testing, you need to ensure that other factors except the model itself are consistent (such as cache, database access, etc.), otherwise the results may be affected. Sample size and test duration also need to be sufficient, otherwise the difference may not be statistically significant. Blue-green deployment needs to be vigilant about **data consistency** issues: if the new version has changes or is incompatible with the database, you need to synchronize data or design migration mechanism during switching. Once switched to the green version, do not immediately delete the blue version before confirming stable, so you can roll back at any time. Whether A/B or blue-green, you should have monitoring and alarm settings: if you find that the new model causes error rate or latency to rise, you should reduce traffic or roll back immediately. Through these strategies, you can reduce the risk of launching new models, controlling potential problems within controllable range.

## Online/Offline Hybrid Architecture

**Definition:** The architecture mode combining online service system and offline batch processing system. In AI applications, usually put the part with high real-time requirements in the online system (Serving), and put the part with large computing but low time requirements in the offline process, thus making full use of resources and optimizing latency. For example, in the classic architecture of recommendation system, the offline stage periodically runs model training and feature processing, and the online stage uses the offline generated model or index for real-time recommendation query.

**Purpose:** For LLM applications, hybrid architecture can be reflected in multiple aspects:
- **Model training/update:** Large model training fine-tune usually offline, then generate model snapshot online for real-time query. Large computing power needed for training does not need to be continuously invested online, just need to complete in offline environment.
- **Batch pre-calculation:** Some inference results can be calculated in advance. For example, for common questions or hot inputs, you can calculate answers in advance and cache them offline, directly return if hit cache online when querying, reducing actual model inference call.
- **Index and retrieval:** In retrieval-enhanced generation (RAG) scenario, first offline encode knowledge base text into vector index, then only perform vector retrieval + small model matching online, saving cost of encoding large text each time.
- **Multi-level architecture:** After front-end online system receives request, it may call multiple modules in the background, some of which are stream online processing, others will task packaged async to offline job. For example, when user submits a request for article summary, system can first return a rough quick summary (online small model), then async call a large model offline to generate fine summary, and update to user after completion.

**Typical implementation:** Common is Lambda architecture (batch processing + real-time processing) application in AI scenario. For example, for summary model that summarizes large amount of social media messages, you can batch fetch and preliminarily classify (offline) every hour, store the result, then use real-time generation on short text when user queries. Technologically, offline part can use big data platform (Hadoop/Spark or more modern Ray Batch), online part use microservice architecture. Both through intermediate storage connection (database, message queue, etc.). An example is search engine question answering: offline stage generates question-answer pairs or embedding index, online through these indexes faster lock related content, then use LLM to generate answer.

**注意事项:** The core of online/offline split is **timeliness and balance**. Need to clearly understand which calculations can be delayed and pre-performed without significantly affecting result timeliness. For pre-calculated content, need to handle data staleness problem (such as knowledge update). You can use periodic rerun offline task or introduce online correction mechanism. In architecture design, need to ensure data format and version consistency between offline output and online model, avoid online model not understanding offline result situation. Finally, monitoring need to cover offline process, if offline task fails or delays, also need to notify operation, otherwise online system may not get updated data and performance degradation. Through collaborative online quick response capability and offline deep computing capability, hybrid architecture can兼顾性能与效果，在教学和真实生产中都十分常用。 