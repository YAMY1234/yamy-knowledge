---
id: cuda-fundamentals
sidebar_position: 1
title: CUDA编程基础
---

## 1. 高效内存管理

**1.1 GPU各类内存优化使用:** CUDA设备上提供多级内存，每种有不同性能特点。**全局内存(Global Memory)** 容量最大，位于显存，所有线程可访问，但延迟高；应通过 **合并访问** 提高带宽利用——即让同一warp的线程连续读取相邻地址，使内存请求合并。**共享内存(Shared Memory)** 是每个SM上片上SRAM，延迟低且带宽高，线程块内共享数据时应优先使用共享内存来缓存热点数据，减少反复访问全局内存。注意共享内存按bank分组并行访问，若一个warp中多个线程访问位于同一bank的地址会发生 **bank冲突** 导致顺序化访问。可通过调整数据布局或在数组末尾添加填充避免不同线程命中同一bank。**常量内存(Constant Memory)** 用于存放只读常量，容量小但每个SM有只读缓存: 对于warp中所有线程读取同一地址的情况，只需一次全局内存读取并广播给整个warp。因此可将模型中的固定权重等放入常量内存，利用其广播缓存优化访存。**纹理内存(Texture Memory)** 是对全局内存的特殊缓存路径，针对2D/3D访问模式做优化，如图像block访问有良好空间局部性时，可利用纹理单元的2D缓存和插值功能。现代架构下，全局内存已有统一的L1/L2缓存体系，对于一般线性访问全局内存已足够，高维访存或需要硬件插值时纹理内存仍有用武之地。

**1.2 CUDA统一内存特性与限制:** 统一内存(Unified Memory)提供CPU和GPU共享的单一内存地址空间，自动进行数据在CPU内存和GPU显存间的迁移，编程更简单。其局限在于性能开销: 按需分页迁移会引入 **分页失效** 和数据传输开销，导致内存访问延迟增加。例如GPU首次访问一段尚在CPU内存的数据时，触发页面错误并由驱动将页面迁移到GPU，这一过程需锁定SM的TLB并等待数据拷贝完成，严重影响并行执行效率。因此，在性能敏感场合需慎用统一内存。可以通过 **显式预取**(如`cudaMemPrefetchAsync`)将数据提前迁移，或使用内存访问提示(如`cudaMemAdvise`)减少页面失效开销。统一内存方便原型和跨GPU内存共享，但在大批量数据场景下，其同步开销和访问延迟可能使性能下降。开发者应权衡易用性与性能，必要时采用手动内存管理(显式`cudaMemcpy`)获得更可控的性能表现。

**1.3 内存访问模式优化:** 优化内存访问模式对性能至关重要。首先要保证 **内存访问合并(coalesced)**: 连续线程访问连续地址，可使硬件将多个访存请求合并为较少的事务，大幅提升总带宽利用率。反之，若线程访问分散(未对齐或跨越不规则地址)，将发生非合并访问，一次warp可能执行多个内存事务，实际带宽利用率和吞吐下降，表现为“未对齐访问”瓶颈。解决方法包括调整数据结构(如使用AoS或SoA布局以配合同步访问模式)，使用`cudaMallocPitch`为二维数据按行对齐分配内存，或确保数组长度为32的倍数以便warp对齐等。其次要注意 **共享内存bank冲突**: Kepler及以后架构共享内存有32个bank，warp中若两个线程访问落在同一bank的不同地址，则访问将串行完成。冲突会严重损耗共享内存性能。避免方法是在共享内存数组中引入 **填充**: 例如将数组长度从32改为33，破坏线程索引与bank的对齐关系，使并发访问分散到不同bank。这种方法常用于需要按warp并行读写共享内存的算法，通过少量内存浪费换取并行性能提升。此外，也可以通过改变索引计算让不同线程访问不同bank。总之，遵循“顺序访问全局内存、避免bank冲突访问共享内存”的模式，并结合 **共享内存分块(tile)** 技术，将大数据划分为小块读入共享内存重复利用。例如矩阵乘法中每个线程块加载A、B的子块到共享内存并多次计算，从而减少全局内存带宽消耗。高效的内存层次利用和访问模式优化可以大幅降低内存瓶颈，实现更高的GPU吞吐。

## 2. 计算优化

**2.1 Warp级原语 (如** `__shfl` **等):** GPU采用SIMT执行模型，一个warp的32个线程束一起执行。CUDA提供 **warp级别原语** 用于warp内部线程高效通信和同步。典型示例是 **warp shuffle** 指令族(如`__shfl_sync`, `__shfl_down_sync`等)，可在无需共享内存的情况下直接让warp内线程互相交换寄存器数据。例如，使用`__shfl_down_sync`可以实现warp内前半部分线程将值发送给后半部分线程，从而在log2(32)=5步迭代内完成一次32元素的归约求和。这种寄存器级数据交换相比将数据写入共享内存再读出要高效得多，因为后者需要执行全局内存地址计算、载入/存储指令以及潜在的同步，而warp shuffle在硬件级完成数据直达传递。“投票”原语(如`__ballot_sync`)则可让warp内所有线程对某个条件表决并返回掩码，用于并行前缀和、计数等操作。同样，`__all_sync`/`__any_sync`用于warp内条件判断，同步掩码用于warp内同步(如`__syncwarp`)等。利用这些warp级原语，开发者可以避免不必要的共享内存使用和线程块同步，在 **归约、扫描、直方图** 等需要线程协作的计算中获得更高效率。例如，在实现warp内归约求和时，通过`__shfl_down_sync`逐步累加，可避免每次加和都访问共享内存，**减少一次加和操作从“1次共享内存读+1次写+同步”缩减为直接的寄存器交换累加**，极大降低延迟。需要注意warp级原语仅限于warp内部线程通信，warp之间仍需借助共享内存或其它机制。总体而言，掌握warp级通信原语并在恰当场景使用，可以充分挖掘SIMT架构潜力，提高计算指令的吞吐。

**2.2 Tensor Core 编程 (使用WMMA API、\_\_hfma指令):** 现代NVIDIA GPU配备的Tensor Core可大幅提升低精度矩阵运算性能(如FP16/BF16/INT8等)，针对深度学习推理中大量矩阵乘法提供数倍于FP32 ALU的吞吐。要利用Tensor Core，开发者有两种途径: **高阶库** 和 **显式编程**。高阶BLAS库(如cuBLAS、cuDNN)会自动在满足条件时使用Tensor Core。例如，将cuBLAS的计算模式设为`CUBLAS_TENSOR_OP_MATH`并使用半精度数据，可以让GEMM在硬件Tensor Core上执行。但在自定义Kernel中，则需使用CUDA提供的 **Warp Matrix Multiply Accumulate (WMMA)** API来编程Tensor Core。WMMA位于`<mma.h>`头文件的`nvcuda::wmma`命名空间中，提供了一组操作warp级矩阵的C++类和函数，包括 **fragment**(片段)数据结构、加载/存储矩阵的函数和矩阵乘累加指令。典型用法是将大矩阵分块为 **16×16** 的小块，让每个warp负责计算一个小块的乘法累加。代码中warp会声明矩阵A、B的fragment(如`wmma::fragment<wmma::matrix_a, 16,16,16, half>`等)以及累加器fragment，然后使用`wmma::load_matrix_sync`从全局内存加载半精度矩阵块到fragment，调用`wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag)`执行矩阵乘法累加(A×B+Acc)。一次`mma_sync`相当于在Tensor Core上完成 $$16\times16$$ 矩阵乘法并将结果累加到accumulator。最后再用`wmma::store_matrix_sync`将结果写回全局内存。通过这种方式，可充分利用Tensor Core的算力，实现比纯FP32核函数高得多的吞吐。需要注意使用WMMA的 **维度和对齐要求**: 例如矩阵维度需是8的倍数，数据类型必须是Tensor Core支持的(如半精度FP16或采用TensorFloat-32格式的FP32)，否则Tensor Core无法介入。不满足条件时代码会回退到常规ALU实现，无法提升性能。此外，Tensor Core执行低精度乘法并在更高精度累加(如FP16乘法+FP32累加)，因此最终结果与标准FP32运算略有差异，需要确保精度损失在可接受范围。

除了WMMA宏观利用Tensor Core外，在微观层面还能使用半精度的 **融合乘加指令**。CUDA数学API提供了诸如`__hfma(half a, half b, half c)`的设备函数，它执行半精度的 $$a\times b + c$$ 并一次性完成舍入。这对应底层的 **HFMA** 指令，可在单个指令中完成半精度乘加，利用GPU原生的FP16运算单元。在需要自行实现部分半精度运算时，使用`__hadd`、`__hfma`等intrinsic可以避免编译器将其拆解为多个指令，从而充分利用硬件的半精度计算能力。例如，在LayerNorm等操作中对half数组进行加法、乘法累计，使用`__hfma`可以减少指令数量并提高速度。总之，开发者应尽量以 **Tensor Core友好的方式** 编写代码: 批量的矩阵计算尽量使用库或WMMA，加减乘等标量运算尽量使用半精度intrinsic，从而在LLM推理的矩阵乘、卷积等热点算子上获得最大加速效果。

**2.3 大模型推理常见算子的高效实现:** 

* **Attention(注意力机制)算子:** Transformer中的多头注意力计算是LLM推理的核心开销之一，涉及查询-键相似度矩阵计算、Softmax归一化、加权求和等步骤。传统实现中，这些步骤分别由独立kernel完成，例如先用GEMM计算 $$Q \times K^T$$ 得到注意力得分矩阵，再用kernel计算Softmax，随后再乘以V。这种多kernel串联不仅中间结果(完整的注意力矩阵)需写入显存，造成大量内存访问，还引入多次kernel launch开销。**FlashAttention** 算法通过 **算法重构和内核融合** 大幅优化了注意力计算。其核心思路是在GPU上按块(tile)计算注意力: 一次处理查询和键的小块(比如64×64)并在片上SRAM完成该块对应的部分Softmax，再累积到输出，避免生成完整的中间矩阵。因为不会把大小为$$N\times N$$的注意力矩阵写回HBM，大大减少全局内存读写量。论文和官方实现显示FlashAttention可将注意力计算的显存读写复杂度从$$O(N^2)$$降为$$O(N)$$，并带来 **2–4倍** 的端到端加速。现在主流深度学习库已普遍采用该算法加速Transformer训练和推理。在实现上，FlashAttention将查询Q、键K、数值V划分成tile块，使用共享内存加载Q,K局部块，计算局部得分并 **边计算边应用Softmax** 的 **在线算法**(Online Softmax)。通过存储局部最大值和累积和避免精度损失，最终得到精确的注意力输出且无需额外内存开销。对开发者而言，这体现了 **内核融合和片上计算** 的重要性: 将计算和内存IO按块交织，在SRAM中完成尽可能多的工作，每块计算完再写回较小的数据。总之，高效注意力实现要尽量 **Fuse**: 一个kernel内完成QK^T、Softmax和加权求和等子步骤，最大程度降低显存流量与kernel启动延迟，从而支持更长的上下文长度下实时推理。

* **矩阵乘法(MatMul)与全连接:** 大型语言模型包含大量矩阵乘法(如Self-Attention中的Q\*K^T，以及FFN前馈层的高维矩阵乘)。这些通常是计算最密集的算子，直接决定推理吞吐。因此优化重点是 **利用硬件最高速路径** 完成矩阵乘累加。首先，应使用 **Tensor Core** 进行计算: 例如将权重和激活采用FP16/BF16存储，利用半精度Tensor Core实现矩阵乘法，加速比可达数倍。如果使用深度学习框架，一般可依赖cuBLAS、cutlass等提供的优化内核(例如GEMM、conv)。对自定义算子实现，则可以采用前述WMMA API手工编写内核，以tile方式让每个线程块/warp负责输出矩阵的一块，从全局内存按行块加载A、按列块加载B到共享内存或fragment，然后使用Tensor Core并行计算。除了精度要求极高的场景，LLM推理通常可接受FP16甚至更低精度，只要配合校准或微调保持模型精度。因此近年也流行 **8比特量化** 推理(INT8或更低)配合特定乘法指令以进一步提升速度和减小显存占用。一些Ampere GPU支持INT8 Tensor Core运算，对应WMMA的整数类型API或使用CUDA的`__dp4a`等指令实现4个INT8乘加。同样重要的是 **内存优化**: 矩阵乘法往往是memory-bound与compute-bound交替。在推理批大小较小时，算子可能未完全利用GPU算力，因此保证矩阵数据在内存中的布局和对齐(例如权重使用 **向量化加载**、避免未对齐访存)也很关键。如果矩阵乘法后接简单的逐元素操作(如Bias加法、激活函数)，可考虑融合进GEMM内核(这就是所谓GEMM-Strided-Bias-Activation融合，在cuBLAS和TensorRT中都有实现)以避免额外的内存往返。总之，大模型推理中矩阵乘法算子的优化手段包括: 使用最低可接受精度的Tensor Core实现、充分向量化内存访问以及融合后处理操作，以实现极限吞吐。

* **Layer Normalization(层归一化):** LayerNorm在Transformer中每层都会使用，其计算包括对输入的一维向量计算均值和方差，然后归一化并缩放平移。它涉及 **按元素的加减乘除** 以及 **一次全局归约**(计算均值/方差)，理论算量不大但需要两次遍历数据(或一次遍历同时计算均值和平方和)。Naive实现可能使用两个kernel: 先计算均值和方差，再第二个kernel完成归一化。这种实现的主要瓶颈在于 **内存带宽**: 每个元素需要读写多次，全局内存访存占主导。为优化LayerNorm，应当 **融合** 这些步骤为单个kernel，一次加载数据即可完成计算。OneFlow提供的高效LayerNorm就是一个例子: 它借鉴Softmax的优化策略，将对每行(normalized\_shape对应的轴)的归约与归一化放在同一个CUDA kernel中，一次性完成。具体来说，该kernel会为每个向量启动足够的线程并行累加计算sum和sum of squares(有时采用Welford算法以保证数值稳定)，利用warp级shuffle在线程内聚合，从而在一次内核中得到均值和方差，然后直接对每个元素计算 $$(x-\mu)/\sqrt{\sigma^2+\epsilon}$$ 并乘以$$\gamma$$加上$$\beta$$输出。NVIDIA的Apex库也提供了融合的LayerNorm内核，用于替换PyTorch中用两个kernel实现的版本。实际测试显示: OneFlow优化的LayerNorm相对Apex版又有提升，在FP16数据下对于不同向量长度都表现出更高的带宽利用率和更低的执行时间。这证明了融合和向量化的重要性: OneFlow的实现对输入做了内存对齐优化(一次加载多个元素)，并充分利用了GPU寄存器/共享内存来减少全局访存。此外，如果GPU支持半精度运算，可以在计算均值方差时使用半精度累加(在精度许可范围内)以加快计算。总之，高效LayerNorm实现要点: **单kernel融合归约和归一化**、使用并行归约算法(如warp shuffle)提高计算效率、利用向量化内存访问提升带宽使用。通过这些手段，LayerNorm这样的内存瓶颈型算子可以获得5～10倍的提速，从而减小对模型整体推理速度的影响。

## 3. 并行性和调度优化

**3.1 CUDA流与异步执行:** CUDA **流(Stream)** 是GPU上一系列按序执行的操作队列。默认情况下，所有CUDA调用在默认流中按顺序执行且彼此阻塞。为了提高并行度，必须创建多个流使不同任务独立执行。**利用多流可以实现计算与数据传输的并发**: 例如，一个流用于执行GPU计算，另一流用于在CPU和GPU间传输数据，只要使用异步内存拷贝，数据传输就可与计算重叠。关键在于使用 **异步API**(如`cudaMemcpyAsync`而非阻塞的`cudaMemcpy`)并确保主机内存是 **固定页内存(pinned memory)** 来允许DMA引擎并行工作。典型案例是在推理pipeline中，流1上执行当前batch的模型计算，同时流2上预取下一batch数据到GPU，这样GPU计算单批的同时其数据传输时间被隐藏在上一批计算周期内。CUDA硬件一般有独立的复制引擎和计算引擎，甚至支持双向并发传输，这使得同时进行H2D传输、D2H传输和内核执行成为可能。通过合理划分任务到不同流并使用事件或`cudaStreamSynchronize`来协调依赖，能实现 **流水线并行**: 例如当处理序列数据时，可将不同时间步的计算或不同模型层的计算放在不同流，只要没有数据依赖就可并行。需要注意避免不必要的流间同步，否则会丧失并发执行效果。另外，可利用流优先级调整不同任务先后顺序，以确保关键计算优先执行。在大模型推理中，多流并行常用于同时服务多个推理请求(多batch并发)、或在多GPU场景下Overlap通信和计算。掌握流的并发执行机制并巧妙安排任务顺序，能够显著提高硬件利用率，减少等待时间。

**3.2 计算与数据传输重叠:** GPU性能常受限于内存传输延迟，通过重叠数据拷贝和计算可以隐藏传输开销。实现方法如上所述，需要使用CUDA流和异步拷贝。具体策略包括: 在GPU执行当前kernel时，提前异步将下一次计算所需的数据从CPU内存拷贝到GPU(H2D)，以及将不再需要的输出结果开始回传主机(D2H)，这样传输在后台进行，当GPU空闲时数据也已准备好。例如，在批量推理场景，可以准备两个Buffer交替使用(双缓冲): 当buffer A的数据正在供GPU计算时，buffer B用于加载下一批数据；计算完成切换时，buffer B的数据已传输完毕，可立即开始计算，同时将buffer A的结果回传主机。为了实现这种Overlap，需要确保 **不同操作放入不同流** 且无显式同步。CUDA会在不同流之间自动寻求并发执行，只要硬件资源允许且操作无冲突。在实际使用中，检查是否真正实现重叠很重要，可借助Nsight Systems时间轴查看计算段和拷贝段是否并行。一般来说，实现Compute/Copy重叠需要满足: 使用了`cudaMemcpyAsync`且目标流非默认流、主机内存已锁页、GPU有独立复制引擎。一旦配置正确，计算和数据传输几乎可以 **同时** 进行，提高流水线吞吐。对于序列推理(如生成长文本)，虽然推理过程本身是串行的，但也可以通过异步预取下一步需要的词嵌入/模型权重块等，使数据准备不阻塞计算。又或者在多路视频/音频处理推理中，各路数据传输和模型推理均可并发。总之，“计算-通信重叠”是高性能GPU程序的重要优化手段，它能够将原本串行的等待时间隐藏，从而降低总延迟、提高设备利用率。

**3.3 Kernel融合与Launch延迟优化:** 每次CUDA kernel启动都有固定的开销(调度延迟，一般级别为数十微秒)，对于大量小Kernel，会造成总开销显著累积。因此优化的一大方向是在保证逻辑正确的前提下 **合并内核调用(kernel fusion)**。**Kernel融合** 指将原本串行执行的多个小kernel逻辑合并在一个kernel中执行。这有两方面好处: (1) **减少全局内存往返**: 中间结果不需要反复写回显存再读出。例如对同一数组依次执行加法、激活两步，与其用两个kernel各读写一次，不如融合成一个kernel，一次读入后直接计算两步并写出一次。一个简单示例: 假设要计算E = (A+B) + D，若分两kernel，先计算C=A+B(需要读取A,B写C)，再计算E=C+D(读取C,D写E)，总计4次全局内存访问；而融合后一次kernel直接计算E = A+B+D，只需读取A,B,D各一次、写E一次，共3次内存访问。内存访问减少意味着更少的延迟和带宽占用。(2) **减少Kernel Launch延迟**: 两个小kernel各有一次启动延迟，融合成一个后启动开销几乎减半。特别在 **短耗时kernel** 场景，launch latency会占显著比例。例如在Transformer中，大量逐元素操作(比如逐元素加减、激活函数等)单独来看非常快，但如果逐个launch会让GPU因调度开销和同步而空转。通过融合，这些操作可一次启动全部完成，大幅提升整体效率。著名的 **FlashAttention** 正是极端的kernel融合案例: 它将注意力的所有子过程(点积、Softmax、加权求和)融为单个kernel，避免了大量中间数据的存取和kernel启动，同样的硬件实现了数量级的加速。

然而Kernel融合也有 **限制**: 如果待融合操作之间没有数据相关性，则无法简单地合并；或者中间结果非常庞大，以至于单个SM的共享内存装不下全部数据，这种情况下也不得不分多步处理(FlashAttention中通过算法创新解决了标准Softmax需要一次处理完整矩阵的问题)。一般经验是融合 **计算密度低、内存带宽受限** 的算子最为有效，例如Transformer中的LayerNorm+BiasAdd+Dropout+Residual Add常被融合为一个多功能kernel执行。GPU编译器和运行时(如PyTorch的`torch.compile`、TensorRT优化器)也在尝试自动kernel融合，但手工优化往往更具针对性。除了融合，**减少launch开销** 的另一个手段是使用CUDA Graph等批处理执行方式，将一系列kernel录制为图后一次提交，避免每次launch的开销和CPU-GPU同步。不过CUDA Graph适用场景有限，仍在持续发展中。

归根结底，在LLM推理优化中应尽量 **减少微小Kernel的数量**。如果模型的算子划分过于碎片(每个算子只做简单操作)，可以考虑使用Fuse技术将多个算子合并。例如将多个逐元素算子或小规模计算算子手工写成一个CUDA kernel。这样既提高单kernel的计算量(提升算术强度)，又避免过多内存写回，**使GPU长时间“忙碌”而非“等候”**。大型语言模型中，这种优化尤为重要，因为Transformer层中很多操作(激活函数、缩放、偏置加和等)计算量相对小但分布频繁。通过融合，这些操作的耗时几乎可以忽略，从而让总延迟更接近理论最低。实践中需注意保持代码可维护性和正确性，但对于追求极致性能的场合，kernel融合是一项不可或缺的技术。

## 4. 性能分析与调优

**4.1 分析工具: Nsight Compute / Nsight Systems / nvprof:** 高效优化需要定量分析瓶颈，NVIDIA提供了多款Profiling工具。**Nsight Systems** 是系统级时间轴分析工具，可捕获CPU和GPU上的事件时序。通过它可直观看到GPU kernel执行时间、重叠情况以及内存拷贝、CPU计算等全局视图，帮助发现例如GPU空闲等待、数据传输未重叠等问题。**Nsight Compute** 则是深入单个GPU kernel的分析器，它通过硬件计数器收集详尽的性能指标，如每个kernel的寄存器使用、共享内存利用率、指令执行情况以及内存层级命中率等。Nsight Compute带有“专家系统”提示，会根据指标给出潜在瓶颈的建议(例如内存吞吐接近上限则提示memory-bound，或Warp执行分支发散则提示branch divergence)。常用Nsight Compute指标包括 **SM效率/执行占比**、**内存吞吐**、**访存命中率**、**Warp执行效率** 等，这些指标可以帮助判断kernel受限于计算还是内存。例如，若观察到某kernel的全局内存读取吞吐接近硬件上限而算术单元闲置，则说明该kernel **受内存带宽限制**。相反，如果内存利用率很低但计算单元也不满，则可能是 **延迟受限**(如长延迟内存访存未被足够的并发隐藏)或者有 **同步等待**。还有 **Achieved Occupancy(实际占用率)** 等指标可以反映线程并发度，若占用率远低于理论最大，意味着线程块/线程数配置不理想或资源(寄存器/共享内存)受限。通过这些工具，调优者可以定位性能瓶颈所在。对于老GPU或简要分析，**nvprof**(CUDA命令行Profiler)也可使用。`nvprof --print-gpu-trace`能打印时间轴上每个kernel和内存拷贝的时间；也可用`nvprof --metrics <metric_name>`采集指定指标，例如`dram_read_throughput`和`dram_write_throughput`评估显存带宽利用。nvprof已被Nsight系列取代，但在无图形界面的服务器上快速定位问题仍有用。

**4.2 利用CUPTI进行Profiling:** 对于高级用户，**CUPTI**(CUDA Profiling Tools Interface)提供了编程接口，可在应用运行时采集GPU的性能事件和指标。CUPTI包含多种API，例如Activity API用于跟踪GPU上的活动(kernel执行、内存拷贝时间等)、Event和Metric API用于读取硬件计数器(如指令发射数、cache命中率)、以及Profiling API可以设置采样范围和收集指标等。利用CUPTI，开发者能构建自定义的Profiling工具或将Profiling嵌入应用程序，实现自动化的性能监测。例如，可以通过CUPTI捕获每次kernel的开始/结束事件，测量持续时间，或读取每次kernel的SM忙碌率、每周期发射的指令数、内存读写带宽等。CUPTI还能分析Unified Memory的迁移、NVLink通信等事件。在可视化方面，开发者可将CUPTI采样的数据导出，例如使用Python API或NVTX标记，将性能数据集成到自定义仪表盘中。需要注意CUPTI的使用会引入一定开销，但其设计注重低开销和确定性。一些开源工具(如 **TensorBoard GPU插件**、**Nsight Systems** 内部)也使用CUPTI作为后端。因此在性能调优中，如果Nsight Compute等无法满足特定需求，可以考虑使用CUPTI自行收集数据。例如大规模部署中用CUPTI定期采样GPU各单元利用率、每层kernel执行时间等，以监控推理效率。总之，CUPTI赋予开发者深入了解GPU行为的能力，对于 **细粒度剖析** 和 **自动化性能调优** 非常有价值。

**4.3 关键性能指标解读(SM占用率、内存吞吐、延迟等):** 在分析工具的报告中，几项关键指标经常决定优化方向: 

* **SM Occupancy (SM占用率)**: 表示GPU Streaming Multiprocessor上实际运行的warp数量相对于可同时运行的最大warp数量的比例。简而言之，Occupancy反映每个SM上的并行度高低。较低的占用率(例如20%或更低)意味着每个SM上同时执行的线程不多，GPU可能有大量闲置计算单元。低占用率可能由线程块规模小、总线程数不足，或寄存器/共享内存用量过高限制了可并发的线程块数。在Memory-bound的kernel中，有时即便占用率低也能把内存带宽打满，但计算单元闲置。一般优化中，如果发现占用率偏低且kernel性能不佳，可尝试增大线程块尺寸、增加并发blocks数量，或优化资源使用来提升Occupancy，从而更好地隐藏内存延迟。然而，占用率并非越高越好，过高占用可能导致寄存器压力增加反而降低每线程性能。因此应结合其他指标全面考虑。

* **Memory Throughput (内存带宽吞吐)**: 衡量kernel在全局内存或各级缓存上的读写带宽利用情况，通常以GB/s或相对于硬件峰值的百分比表示。如果一个kernel的DRAM带宽利用接近GPU硬件上限(例如利用了90%以上带宽)，而计算单元利用率不高，基本可判断该kernel受限于内存带宽，即 **内存绑定**。优化这类kernel要从减少内存流量、提高访存效率入手，例如改善访存模式(合并、对齐)、增加缓存命中或采用更高算术强度的算法。相反，若内存带宽利用率很低，而SM也不忙，则说明GPU既未满载计算也未满载内存，**延迟因素** 可能是瓶颈(如串行依赖、同步、访存延迟未被并行隐藏等)。需要进一步查看 **Warp Stall原因** 等指标判断是等待内存(memory latency)还是其它。在Nsight Compute中，可以看到L2吞吐、缓存命中率: 比如某kernel L2命中率低而DRAM流量高，则可以考虑引入软件缓存(共享内存)或调整访问模式。

* **延迟与流水线停顿**: GPU采用深度流水线并行执行，但各种原因会造成warp停顿(stall)。Nsight Compute提供 **Warp Stall 分析**，列出了warp因何种原因暂停执行的时间占比，如 **内存依赖(memory throttle)**、**执行依赖(execution dependency)**、**同步等待**、**管线忙** 等。这些数据有助于分析延迟瓶颈。例如，若“内存依赖”stall比例很高，说明warp常常在等待内存数据返回，此时提高并发(更多warps调度)或优化访存(如使用寄存器重用、减少长latency访存次数)是方向。如果“执行依赖”占比高，表示指令间依赖导致流水线等待，可以尝试重排计算顺序或插入独立指令等。对于大模型推理，一般算子规模较大，GPU可用大量并行度隐藏大部分延迟，但在小batch或序列长度较短情况下，也可能出现单warp工作量小、切换不充分导致延迟瓶颈。通过分析这些stall原因，可以针对性地优化，例如引入双缓冲技术减少同步等待、利用更多寄存器减少重复访存等。

* **SM效率和利用率**: 一些Profiler报告“SM效率”或“SM使用率”，这与Occupancy有相关但不同。它通常指GPU有多少比例的SM在某段时间内在执行指令。例如GPU有80个SM，如果一个kernel只使用了其中20个(因为线程块数有限)，那么SM效率仅25%。这在多流或多并发kernel场景下重要: 我们希望尽量让所有SM都参加计算。改进方法包括增加并行任务数，或者使用Multi-Stream并发，以免部分SM空闲。**GPU利用率**(nvidia-smi显示的)则更笼统，表示GPU有没有在执行任务的时间比例。如前述，当只用1个SM跑kernel时，GPU利用率100%但SM效率可能不到2%。因此调优时应该更关注SM级指标。为监测整个应用的效率，可使用NVIDIA的DCGM工具，它能提供例如每秒FP16/INT8运算数、内存读写字节数等综合指标。例如某LLM推理任务，观察到FP16 TensorCore利用率只有30%，则说明还有优化空间(也许因为剩余时间都在做内存访问或等待)。目标应是让 **关键硬件单元**(算术单元或内存通道)尽量接近饱和，但又不超载导致队列堆积。这个平衡需要借助上述指标不断试验调整。

通过上述指标的联合分析，调优者可以 **定位瓶颈**(计算 vs 内存 vs 延迟)、**评估优化效果**(例如修改后SM Occupancy是否提高，内存带宽占用下降等)以及 **指导下一步措施**。总的来说，大型模型推理的理想状态是: **大算子** 计算时Tensor Core/ALU高负载，小算子或内存相关步骤尽量隐藏在并发或流水线中，使GPU始终以接近峰值效率运行。

## 5. 架构差异性简述

**5.1 Ampere vs Hopper 架构主要差异:** NVIDIA Ampere架构(如A100 GPU)和后继的Hopper架构(如H100 GPU)在硬件特性上有多项区别，对LLM推理优化策略产生影响。

* **Tensor Core精度与Transformer Engine:** Ampere的Tensor Core属于第三代，支持FP16、BF16、TensorFloat32(TF32)和INT8等运算。TF32是一种针对FP32运算兼容的格式，使用10位有效位在Tensor Core上执行，提供与FP32接近的精度但大幅加速训练。Ampere还首次引入 **结构化稀疏** 支持: 如果矩阵满足2:4结构化稀疏模式，可在Tensor Core上实现理论上2倍的速度提升(因为有效计算元素减半)。在推理中，如果模型经过稀疏化剪枝，这一特性可以被利用。然而稀疏模式要求严格，在大多数LLM推理实践中未广泛采用。**Hopper架构** 引入了第四代Tensor Core和全新的 **Transformer Engine**。Transformer Engine可以 **动态选择FP8低精度计算**: H100的Tensor Core支持FP8格式(两种混合精度E4M3和E5M2)，其理论吞吐是FP16的两倍。Transformer Engine在软件上可根据张量的分布动态调整FP8的比例位宽，以减小精度损失。NVIDIA报告H100通过FP8在Transformer模型上达到数倍于A100的性能提升，例如官方宣称H100对大模型推理最高可达A100的30倍速度(这通常是在利用FP8和更高密集度硬件上的综合提升)。因此，在Hopper上优化LLM推理时，应尽量使用FP8精度(如果模型精度允许)来获取最大性能。而Ampere不支持FP8，只能用FP16/BF16。总的来说，Hopper在低精度计算上的优势使其更适合大模型的高吞吐推理。

* **SM架构与缓存:** Ampere A100 GPU包含108个SM(多流处理器)，每个SM具备最多64个并发warp的调度能力。A100的每SM拥有 **共享内存/L1缓存统一架构**，容量上限为164KB左右(具体取决于L1和共享内存的划分)。Hopper H100的SM数量提升到132个(SXM版)，每个SM的L1/共享内存总容量增加到 **256KB**。这意味着H100在片上缓存/共享存储方面比A100提高了约50%，有助于容纳更大的tile块或更多数据重用，从而减少对慢速显存的访问。此外，Hopper的L2缓存也从A100的40MB增至50MB，并使用更快的HBM3显存带来高达3.35TB/s的内存带宽，而A100(80GB版) HBM2e带宽约2.0TB/s。因此，在H100上，即使模型相同，**缓存命中率和可用带宽** 均提升，推理时可以容纳更长序列长度的数据在缓存中、减少显存瓶颈。在优化策略上，Ampere时期往往需要更严格地tile数据计算以避开较小的片上存储限制，而在Hopper上可以利用更大的共享内存一次处理更大块的数据(例如FlashAttention在H100上可用更大tile以降低Softmax分块次数)。同时，更大的L2也利于缓存权重和激活，从而 **降低DRAM访存压力**。

* **新硬件功能:** Hopper引入了一些Ampere没有或不完善的硬件特性。首先是 **Tensor Memory Accelerator (TMA)**。Ampere架构已提供`cp.async`指令允许异步将全局内存加载到共享内存，但需要手动管理同步。Hopper的TMA进一步增强了异步内存传输能力: 它可以在硬件级别处理多维(最多5维)张量的数据传输，包括从共享内存到全局内存的写回，并支持在传输过程中执行按元素的简化运算(如聚合、位操作)。TMA由新的`cuda::memcpy_async`接口暴露，使用它可以极大减少长距离内存拷贝对SM计算单元的干扰。例如在FlashAttention-2升级到3的工作中，就利用了H100的TMA在专门的loader warp上预取下一块数据到共享内存，与计算warp重叠执行，实现了 **计算与数据搬运完全并行**。Ampere上虽然也能手工通过双buffer和`cp.async`实现类似重叠，但Hopper硬件使其更加高效、编码更简洁。其次，Hopper增加了 **Warp-Group Matrix Multiply (WGMMA)** 指令。Ampere及之前每个warp的Tensor Core操作彼此独立，无法跨warp协同利用。如果矩阵规模较大，需要靠软件将多warp结果拼起来。而WGMMA允许多个warp组成warp组共同执行一个更大的矩阵乘运算，从而提高Tensor Core单元利用率。据报道，在H100上旧的mma.sync指令只能达到理论FLOPS的约2/3，而使用新的WGMMA可以接近满载。这对于大矩阵(比如非常宽的全连接层)或需要极致性能的情况非常重要。在实际优化中，意味着针对H100可以编写/使用经过优化的新内核(如Cutlass库新版本已支持warp-specialized的WGMMA实现)以获得比A100上相同算法更高的效率。除了TMA和WGMMA，Hopper还支持 **线程块集群** 和 **分布式共享内存**: 多个SM可组成簇共享彼此的SMEM，使跨SM通信延迟降低。这对需要大规模并行归约的算法(如自注意力Softmax归约)可能带来好处，但使用起来较复杂，需要设置集群维度和利用`cuda:: cooperative_groups`等接口。Ampere不支持跨SM直接共享内存，只能依赖全局内存或原子。此外，Hopper对原子操作、分支指令等也有所优化，并增加专用的 **DPX指令** 加速特定算法(如生物信息学Smith-Waterman比对)——这虽非LLM任务，但体现了架构在专用计算上的扩展。

**5.2 不同硬件上的LLM推理优化策略差异:** 考虑上述差异，我们在针对Ampere(A100)和Hopper(H100)优化大模型推理时会有所侧重: 

* **精度与算力利用:** 在A100上主要使用FP16/BF16进行推理，重点关注Tensor Core占用和稀疏利用。如果模型可接受，打开2:4稀疏可以提速，但需要模型预训练支持稀疏结构。而在H100上，应尽量利用FP8 Transformer Engine。优化流程上可能增加一步离线的FP8校准/微调，以确保模型在更低精度下仍可靠，然后在推理时获得最高的算力利用。另外，H100的FP8单卡显存虽与A100类似(80GB)，但因为表示精度低，每层激活/权重占用更小，可以放得下更大的模型或更长序列，这也为推理性能和批处理带来优势。

* **算子实现选择:** 针对H100的新指令和更大缓存，开发者可以选用新算法内核。例如NVIDIA发布的 **FlashAttention++**(FlashAttention-3)就是专为H100设计，利用了WGMMA和TMA实现softmax与GEMM重叠执行。在A100上则可能需要使用FlashAttention-2那样的优化版本(没有依赖WGMMA)。再如GEMM内核，Cutlass在Ampere上以每thread block 128×128等尺寸为主，而在Hopper上可能调整tile以充分利用256KB片上内存和WGMMA。调优者应关注NVIDIA发布的针对不同架构的 **tuning guide**，例如Hopper优化指南建议使用L2持久化API来控制L2缓存驻留等(H100允许程序指定将特定数据长驻L2，以减少反复加载)。这些新功能在A100上不可用。

* **多GPU并行策略:** 若在多卡上推理，H100支持PCIe 5.0和第4代NVLink，带宽更高，可以更高效地分担模型(如张量并行)。Hopper还支持第二代MIG(Multi-Instance GPU)和Hyper-Q改进，可同时服务更多并行推理任务而互不干扰。因此在H100集群上，可以更大胆地增加并发请求数或将单模型拆分到更多GPU而仍保持通信高效。而A100在这方面稍逊，推理并行度过高可能受限于PCIe 4.0或NVLink带宽。硬件差异也意味着 **功耗热设计** 不同: H100 SXM功耗高达700W，A100约400W，H100需要充分跑满才能达到性价比优势，否则能效会低于预期。所以在调度上，H100集群可能更适合大批量长序列推理任务，而A100可用于中等负载以更好能效运行。NVIDIA官方给出的对比是: H100在AI推理上对大模型性能远超A100(如推理吞吐每卡提高约2倍以上)，但前提是用好了FP8等特性，否则提升有限。因此，软件上需要使用NVIDIA的TransformerEngine库或支持FP8的推理框架，在H100上真正激活那些性能潜能。

概括来说，Ampere时代的优化着重于 **半精度Tensor Core** 和 **充分并行隐藏延迟**，而Hopper时代除了继承这些原则，还要掌握 **低精度(FP8)**、**硬件异步加速** 等新特性。针对不同架构进行有针对性的优化，才能充分发挥各自硬件在LLM推理上的性能: A100上尽可能提高计算单元利用、减少访存；H100上则充分利用Transformer Engine和新硬件以达到前所未有的推理速度。

## 6. 实战示例: 使用Tensor Core加速矩阵乘法

下面以一个简单的矩阵乘法CUDA核函数示例，展示如何使用WMMA API利用Tensor Core进行计算加速(假设矩阵大小已按16的倍数对齐，使用半精度FP16输入，FP32累加): 

```cpp
#include <mma.h>
using namespace nvcuda;  // 包含WMMA API

// 每个线程块计算C矩阵的一个 TILE_BLOCK 大小的子矩阵
__global__ void wmmaMatMul(const half *A, const half *B, float *C, 
                           int M, int N, int K) {
    // 计算本thread block负责的输出子块坐标
    int blockRow = blockIdx.y;
    int blockCol = blockIdx.x;
    // 利用WMMA定义片段（fragment）
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> aFrag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> bFrag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> accFrag;
    wmma::fill_fragment(accFrag, 0.0f);  // 累加器清零

    // 每个线程块内部再细分为warp级计算tile
    int warpM = threadIdx.y;
    int warpN = threadIdx.x;
    // 计算当前warp在全局矩阵中的起始元素索引
    int globalRow = blockRow * 16 + warpM * 16;
    int globalCol = blockCol * 16 + warpN * 16;
    // 遍历K维度，分段加载A、B子矩阵块进行累乘
    for (int i = 0; i < K; i += 16) {
        // 将A和B的片段加载到寄存器fragment
        wmma::load_matrix_sync(aFrag, A + globalRow * K + i, K);
        wmma::load_matrix_sync(bFrag, B + i * N + globalCol, N);
        // 进行矩阵乘累加：accFrag += aFrag * bFrag
        wmma::mma_sync(accFrag, aFrag, bFrag, accFrag);
    }
    // 将计算结果存回C矩阵对应位置
    if (globalRow < M && globalCol < N) {
        wmma::store_matrix_sync(C + globalRow * N + globalCol, accFrag, N, wmma::mem_col_major);
    }
}
```

上述代码中，每个warp利用WMMA API实现了 $$16\times16$$ 矩阵块乘法并累加到`accFrag`中。通过`wmma::load_matrix_sync`/`mma_sync`/`store_matrix_sync`，GPU的Tensor Core执行了高吞吐的半精度矩阵乘运算。与常规CUDA实现相比，这种利用Tensor Core的方式可极大提高矩阵乘法算子的推理性能。实际应用中，需根据矩阵规模调整线程块和warp映射策略，并确保数据对齐和类型匹配以充分发挥Tensor Core威力。

以上示例演示了如何在CUDA内核中直接使用WMMA编程Tensor Core。在大型语言模型推理中，类似的原理可用于定制内核(如FlashAttention中的GEMM-Softmax融合kernel)以获得极致性能。通过将本文介绍的各方面进阶技巧融会贯通，开发者可以针对有一定基础的CUDA代码进一步优化，在大模型推理任务上充分挖掘GPU硬件潜能，实现 **更高吞吐、更低延迟** 的推理性能。
