---
id: training-infrastructure
sidebar_position: 1
title: 训练基础设施
---

## 训练基础设施

### GPU/TPU资源管理

### **定义:**

指在多用户、多任务环境下有效分配和利用GPU/TPU算力的机制。GPU/TPU属于昂贵且有限的加速资源，需通过调度策略避免空闲或过载。

### **用途:**

确保训练任务获得所需算力，提高整体利用率并降低成本。例如在Kubernetes集群中，可将GPU作为可调度资源管理，以避免GPU长期闲置或等待。通过动态调度，可按需增加或回收GPU资源，应对负载变化。

### **典型工具/实现:**

在HPC场景下常用Slurm等作业管理器识别并跟踪GPU使用；在云环境下使用Kubernetes Device Plugin或NVIDIA Mig(多实例GPU)支持GPU资源按粒度划分共享。一些方案允许GPU按时间片或显存片段分享给多个任务(如NVIDIA MPS、多租户隔离)以提高利用率。

### **注意事项:**

需考虑GPU异构性和任务需求匹配，例如高显存任务分配到大显存卡，延迟敏感任务选低延迟互联硬件。多租户共享GPU时还要保证隔离，防止资源争抢和数据泄露。此外，动态调度增加灵活性的同时应平衡频繁调度造成的额外开销和潜在延迟影响。

### 训练调度系统(如Slurm、Kubeflow、Ray)

### **定义:**

训练调度系统负责在集群上规划和分配训练作业，管理作业队列、资源分配与执行。它抽象出计算资源供用户提交训练任务，并处理并发调度、优先级和失败重试等。

### **用途:**

当有大量训练任务或需要多机协同训练时，调度系统确保资源高效利用并避免人工分配冲突。例如，Slurm在超算集群中广泛用于队列作业提交与节点分配，支持优先级和分区隔离。Kubeflow基于Kubernetes，可编排机器学习工作流与分布式训练作业。Ray则提供灵活的分布式计算框架，可在其集群上调度Python任务、训练作业，并支持自动扩缩容。

### 典型工具/实现
- **Slurm:** 开源的集群管理和作业调度系统，容错性强、可扩展，很多超级计算机都采用Slurm管理GPU集群。用户通过sbatch提交训练脚本，由Slurm分配节点并并行运行。Slurm重视批处理、优先级队列等，适合固定资源集的高性能训练。
- **Kubeflow:** Kubernetes原生的机器学习平台，提供如TFJob、PyTorchJob等CRD来自定义训练作业。Kubeflow抽象PS-Worker(参数服务器)模型，实现Pipeline工作流，支持超参数搜索和Notebook等。其Training Operator协调分布式训练，利用K8s调度和扩展能力高效运行模型训练。
- **Ray:** 开源统一计算框架，支持将Python/AI工作负载轻松扩展到多节点。Ray自带调度器和资源管理，可在Slurm或K8s上运行。Ray Serve亦可用于部署服务。Ray特别适合需要编程灵活性的场景，提供RaySGD、Ray Tune等组件整合训练、调优和服务功能。

### **注意事项:**

不同系统适用场景不同:Slurm偏重批处理训练，配置固定集群资源，学习曲线高但对深度学习训练较友好；Kubeflow适合云上多用户多流程的ML平台，但部署和维护相对复杂；Ray强调易用性和通用性，但需要注意调度策略以避免资源竞争。选择时需考虑集群规模、用户熟悉程度和生态支持等因素。

### 数据加载与预处理优化

### **定义:**

指为大规模训练准备数据管道以高效读取和处理数据，使GPU保持忙碌，避免因I/O或预处理延迟而造成等待。包括多线程/多进程加载、预取(prefetch)、缓存和数据格式优化等。

### **用途:**

在深度学习训练中，数据管道性能常成为瓶颈。优化数据加载可以提高每秒迭代数，充分利用GPU算力。典型做法是使用并行的数据加载器(如PyTorch DataLoader、TensorFlow tf.data)，启用多个工作线程/进程并提前预取下一个batch的数据至内存或显存。同时可采用内存映射文件或高效数据格式(如TFRecord、WebDataset)减少解析开销。NVIDIA DALI库甚至将数据预处理搬到GPU上，以加速图像/视频等数据增强过程。

### 典型工具/实现
- **多线程/多进程DataLoader:** 例如PyTorch的DataLoader通过worker子进程并行加载batch，并使用pin memory等加速CPU到GPU的数据拷贝。TensorFlow的tf.data也支持`map`并行处理、`prefetch`提前取数据等。
- **GPU加速预处理:** NVIDIA DALI是一个GPU加速的数据加载与预处理库，可利用GPU执行图像解码、增强等，并支持多batch缓冲，减少GPU等待数据时间。在使用DALI时，可将多个batch数据缓冲，隐藏不同batch预处理时间差异。
- **高效数据格式:** 将原始数据转换为顺序存储的二进制格式(如TFRecord、LMDB)，可以显著提升I/O吞吐。同样，Hugging Face的datasets库提供内存映射加速。安全tensor存储格式(如Safetensors)在模型加载中亦有作用。

### **注意事项:**

优化数据管道需平衡CPU和GPU资源。例如增加DataLoader进程数提高加载速度，但过多进程可能导致CPU争用或内存不足。应监控GPU利用率，调整batch大小、预取buffer大小等使GPU计算与数据准备重叠。对于分布式训练，常要确保每个GPU读取不同数据片段以避免重复(如Sampler按rank划分数据)。另外，网络I/O场景下考虑本地缓存数据或分布式文件系统的吞吐，必要时可使用NVMe SSD作缓存。总之，目标是将数据等待时间降至最低，使训练pipeline高吞吐持续运行。

### 大规模数据并行(Data Parallelism)

### **定义:**

数据并行指将**完整的模型副本**复制到多个设备上，各设备处理不同的输入数据分块并独立计算梯度，随后将各自梯度在设备间汇总平均(通常通过AllReduce)，再同步更新模型参数。这样每个设备都训练同一模型的一部分数据，从而加快整体训练进度。

### **用途:**

当数据集极大或训练需要加速时，数据并行是一种常用策略。它在模型能够放入单张GPU显存的前提下，通过N个设备并行处理，使训练近似加速N倍。例如将一个batch拆分到8块GPU各自计算，再汇总梯度更新，相当于批量大小扩大8倍进行同步SGD。数据并行保持了计算逻辑简单性且易于伸缩，因此被广泛用于大规模模型训练(如数千GPU的同步更新)。

### **典型工具/实现:**

主流深度学习框架提供了数据并行支持，如PyTorch DistributedDataParallel (DDP)、Horovod(Uber开源框架)等。Horovod使用环形AllReduce算法高效平均各GPU梯度，PyTorch DDP利用NCCL通信在GPU间同步梯度。FullyShardedDataParallel (FSDP)等也本质是特殊优化的数据并行。谷歌的TensorFlow和JAX亦支持MirroredStrategy或pmap实现数据并行。

### **注意事项:**

数据并行需要每个设备存有完整模型参数，所以当模型非常大时会受限于单设备内存。此外通信开销是瓶颈:每一步需通信梯度，随着GPU数增加，AllReduce耗时增长，不可线性扩展。为减轻此问题，可采用分层AllReduce、压缩梯度、重叠通信与计算等优化。另一个问题是**批归一化**等与batch大小相关的操作在多GPU时需特别处理(同步BN等)。但总体而言，同步数据并行能保证与单机训练等价的更新，在精度一致性方面效果良好。

### 模型并行(Model/Expert Parallelism)

### **定义:**

模型并行是指将**同一模型**按层或计算拆分到多个设备上执行，使单个模型可以跨设备协作计算。当模型体积过大单卡容不下时，这种并行方式必需。主要形式包括**张量并行**(拆分单层矩阵计算)、**流水线并行**(将不同层部署在不同设备，按微批次流水处理)，以及**专家并行**(Mixture-of-Experts，MoE，将不同专家子模型放在不同设备，并对不同数据路由)。

### **用途:**

当模型参数规模远超单卡显存时(如百亿/万亿级参数LLM)，模型并行允许将模型切分后放入多卡，共同完成一次前向/反向计算。例如Tensor Parallel把一个大的矩阵乘法在多卡上分别计算部分乘积再合并；Pipeline Parallel把模型层序列分段，每张卡负责一段层的计算，通过分批输入实现多卡流水工作。这些手段结合数据并行可实现3D并行，支撑超大模型训练。MoE专家并行则通过稀疏激活，使每个token只经过部分专家网络，从而在多GPU上扩展模型容量而推理开销相对小。

### 典型工具/实现
- **Megatron-LM**(NVIDIA)实现了张量并行+流水线并行的结合，可在数百GPU上训练百亿参数Transformer。其Tensor Parallel按行/列切分矩阵运算，Pipeline Parallel按层分组，并通过插入通信bubble等提高流水利用率。
- **DeepSpeed**(微软)支持ZeRO技术减少内存占用，也提供Pipeline Parallel和MoE并行(DeepSpeed-MoE)支持，将专家网络分布到多GPU并用路由算法动态调度token到对应专家。PyTorch的本地实现如`torch.distributed.pipeline`等亦可用。
- **Mixture-of-Experts并行:** 例如Google的Switch Transformer、PyTorch的 Mixture-of-Experts实现等，将多个FFN子网络(专家)分散到不同GPU，仅激活其中一部分。PyTorch官方博客介绍的专家并行(Expert Parallelism)通过all-to-all通信路由token到相应GPU专家，再收集结果。这降低了每个GPU计算负担，却增加了通信和调度复杂度。

### **注意事项:**

模型并行会引入**同步等待**问题:如流水线并行需要平衡各段计算以减少空闲(可用micro-batch填充流水)。张量并行需大量跨设备通信(每层都会通信局部结果)，对高速网络要求高。专家并行则有路由不均衡和通信复杂的问题(需要all-to-all分发token)。通常训练框架会结合数据并行一起使用(混合并行)以获得更佳效率。开发者需根据模型结构选择合理的并行策略并调优通信和计算重叠，以减少并行带来的效率损失。

### 分布式训练框架(DeepSpeed、Megatron-LM、FSDP、Horovod等)

### **定义:**

这些框架提供上层封装和系统优化，帮助用户更容易地在多GPU/多机环境下训练超大模型，同时充分利用内存和通信带宽。它们往往集成多种并行策略和优化技术，使得"单机代码"几乎无需修改就能扩展到集群。

### 典型工具/实现
- **DeepSpeed:** 由微软推出的深度学习优化库，能够便捷地实现大模型的高效训练。DeepSpeed的核心是ZeRO策略，通过将优化器状态、梯度等在各GPU间分片存储，消除数据并行中的内存冗余。借助ZeRO Stage 3，每张GPU只保留模型一部分参数和状态，从而支持百亿甚至上千亿参数模型在有限显存GPU上训练。此外DeepSpeed还提供混合精度训练、梯度检查点、通信重叠等性能优化和自动并行配置调优。简单调用`DeepSpeed.init`即可包装PyTorch模型享受这些加速。
- **Megatron-LM:** NVIDIA开源的超大Transformer训练框架，针对Transformer架构进行了深度优化。Megatron-LM实现了高性能的模型并行(张量并行+流水线并行)，并融合了NCCL通信优化和高效的LayerNorm、KV缓存管理等。在LAMB优化器和大batch配合下，Megatron-LM曾用于在短时间内训练GPT-3级别模型。其缺点是定制较深，需要按其提供的模型定义方式编写。
- **FSDP (Fully Sharded Data Parallel):** PyTorch官方提供的分布式并行方案，将模型参数、梯度和优化器状态全部切分 shard 到不同GPU，仅在需要时通信重建。这与ZeRO-3类似，可以视作PyTorch原生的ZeRO实现。使用FSDP，开发者基本按普通模型编写代码，然后用`torch.distributed.fsdp.wrap`包装模块，即可实现在多GPU的完整内存优化数据并行训练。FSDP相较传统DDP在大模型场景下显著节省显存，训练更大模型成为可能。
- **Horovod:** Uber开源的跨框架分布式训练工具，支持TensorFlow、PyTorch、Keras等。Horovod核心是基于MPI的AllReduce算法，使用环形AllReduce来平均梯度，从而避免参数服务器瓶颈。它的优点是易用，只需在训练脚本中少量修改(如用`hvd.allreduce`替换梯度更新)即可扩展单机代码到多机，多框架统一接口也降低了学习成本。Horovod展示了接近线性加速效果。但Horovod需要MPI和NCCL支持，对网络要求较高，通常在专用GPU集群或高速互联环境下使用。

### **注意事项:**

使用这些框架需关注版本兼容和配置调优。例如DeepSpeed需要匹配的PyTorch版本和NCCL版本，首次使用需仔细设置config以启用各优化项(如ZeRO stage等)。Megatron-LM等可能对硬件拓扑有假设(如要求GPU NVLink全互联)。FSDP目前正快速迭代中，某些PyTorch层可能不兼容(需要wrap处理)。Horovod需要确保MPI环境正确配置，尤其是在容器内或云环境下。总体来说，这些框架为大模型训练提供了强力工具，但充分发挥其效能仍需结合具体场景进行参数调节和profiling监控。

### Checkpoint/Restart 机制

### **定义:**

分布式训练的checkpoint机制指定期将模型当前参数和训练状态保存到存储介质，以便在作业中断或结束后能够**复现或恢复**训练进度。一个完整的checkpoint通常包含模型权重、优化器状态(如动量、学习率调度器状态)、随机数种子、已训练的步数/epoch等信息。Restart则指当训练由于某种原因停止后，从最近的checkpoint加载这些状态，继续训练而不损失已完成的计算进度。

### **用途:**

在大型模型长时间训练(可能持续数周)的情况下，checkpoint至关重要:它提供了故障恢复能力(Fault Tolerance)。例如在使用数百GPU训练LLM时，任何节点故障都可能终止作业，但通过checkpoint可以从上一次保存点继续，而不需要从头开始。同时checkpoint也用于保存中间成果，以供后续微调或推理使用(如保存每轮epoch模型快照)。

### **典型工具/实现:**

主流框架皆有checkpoint支持。PyTorch可以用`torch.save`保存`state_dict`，或使用训练引擎(如Trainer)自动定期checkpoint。分布式环境下，可让每个进程保存本地rank的数据，或由主进程收集统一保存。更高级的，如PyTorch `torchrun`提供弹性训练特性:自动捕捉失败并重启所有进程，从最后一个快照(snapshot)恢复。DeepSpeed也内置了模型并行情况下的一致性checkpoint方案，Horovod则建议使用其自带的`hvd.broadcast()`确保各进程恢复同步。

### 注意事项
- **频率与性能:** Checkpoint过于频繁会影响训练性能，因为写磁盘(尤其分布式文件系统)耗时不小。通常根据训练时间设置间隔(如每小时或每若干steps)。此外可以启用异步写入(后台线程写checkpoint)以减少阻塞。
- **存储与大小:** 大模型checkpoint可能数十GB，需有足够的高速存储空间。采用增量存储或压缩格式(如Safetensors、FP16存储)可减小大小。
- **一致性:** 恢复时必须确保每个GPU加载对应分片参数和优化状态。使用FSDP/ZeRO时checkpoint需要包含分片信息并在重启时正确加载。
- **容错训练:** 有些新系统支持更细粒度的故障恢复，如深度速度的弹性聚合或谷歌的Checkpoint机制等。总体而言，实现容错需权衡保存频率与开销。一个案例是通过优化镜像和模型权重加载，将某模型冷启动时间由6分钟降到40秒，这极大提高了在有故障或扩容场景下的响应速度。 